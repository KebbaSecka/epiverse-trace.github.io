[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "This blog is also available on R-bloggers.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nsuperspreading v0.1.0\n\n\n\n\n\n\n\nnew-release\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2023\n\n\nThe Epiverse-TRACE development team\n\n\n\n\n\n\n  \n\n\n\n\nCommunicating development stages of open-source software\n\n\n\n\n\n\n\nopen-source\n\n\nR\n\n\nR package\n\n\nversioning\n\n\nlifecycles\n\n\ncommunity\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2023\n\n\nJoshua W. Lambert\n\n\n\n\n\n\n  \n\n\n\n\nSharing the C++ Code of an Rcpp Package\n\n\n\n\n\n\n\ncode sharing\n\n\nR\n\n\nR package\n\n\nRcpp\n\n\ninteroperability\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2023\n\n\nPratik Gupte\n\n\n\n\n\n\n  \n\n\n\n\nExtending Data Frames\n\n\nCreating custom classes and {dplyr} compatibility\n\n\n\n\ndata frame\n\n\nR\n\n\nR package\n\n\ninteroperability\n\n\nS3 class\n\n\ndplyr\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2023\n\n\nJoshua W. Lambert\n\n\n\n\n\n\n  \n\n\n\n\nWhat Should the First 100 Lines of Code Written During an Epidemic Look Like?\n\n\n\n\n\n\n\noutbreak analytics\n\n\n100 days workshop\n\n\nparticipatory research\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nCarmen Tamayo\n\n\n\n\n\n\n  \n\n\n\n\nConvert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations\n\n\n\n\n\n\n\nR\n\n\nR package\n\n\nobject orientation\n\n\nS3\n\n\ninteroperability\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nHugo Gruson\n\n\n\n\n\n\n  \n\n\n\n\nImproving the C++ Code Quality of an Rcpp Package\n\n\n\n\n\n\n\ncode quality\n\n\nR\n\n\nR package\n\n\nRcpp\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\nPratik Gupte\n\n\n\n\n\n\n  \n\n\n\n\nEnsuring & Showcasing the Statistical Correctness of your R Package\n\n\n\n\n\n\n\ncode quality\n\n\nR\n\n\nR package\n\n\ntesting\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nHugo Gruson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/share-cpp/index.html",
    "href": "posts/share-cpp/index.html",
    "title": "Sharing the C++ Code of an Rcpp Package",
    "section": "",
    "text": "Using the {Rcpp} package is the dominant method for linking the usability of R with the speed of C++, and can be used to write R packages that are fast and easy to use for both end-users and developers.\nFrom the point of view of developers, it’s very easy to export R code such as functions and classes from an R(cpp) package, but the guidance in the Rcpp documentation does not detail how to export the C++ code so that it can be shared with your other Rcpp packages.\nAllowing C++ code to be shared can be very beneficial for the same reasons that sharing R code is — packaging code is a reliable way to reuse it.\nSome widely used examples of this practice are the {RcppEigen}, {RcppArmadillo}, {RcppGSL}, and Boost Headers {BH} packages. Indeed, in the Epiverse-TRACE team, {RcppEigen} underpins the {finalsize} and upcoming {epidemics} packages."
  },
  {
    "objectID": "posts/share-cpp/index.html#why-share-c-code-from-an-rcpp-package",
    "href": "posts/share-cpp/index.html#why-share-c-code-from-an-rcpp-package",
    "title": "Sharing the C++ Code of an Rcpp Package",
    "section": "",
    "text": "Using the {Rcpp} package is the dominant method for linking the usability of R with the speed of C++, and can be used to write R packages that are fast and easy to use for both end-users and developers.\nFrom the point of view of developers, it’s very easy to export R code such as functions and classes from an R(cpp) package, but the guidance in the Rcpp documentation does not detail how to export the C++ code so that it can be shared with your other Rcpp packages.\nAllowing C++ code to be shared can be very beneficial for the same reasons that sharing R code is — packaging code is a reliable way to reuse it.\nSome widely used examples of this practice are the {RcppEigen}, {RcppArmadillo}, {RcppGSL}, and Boost Headers {BH} packages. Indeed, in the Epiverse-TRACE team, {RcppEigen} underpins the {finalsize} and upcoming {epidemics} packages."
  },
  {
    "objectID": "posts/share-cpp/index.html#two-ways-to-share-c-code",
    "href": "posts/share-cpp/index.html#two-ways-to-share-c-code",
    "title": "Sharing the C++ Code of an Rcpp Package",
    "section": "Two ways to share C++ code?",
    "text": "Two ways to share C++ code?\nDevelopers searching for a way to make the C++ code of their Rcpp-based packages shareable will likely find two seemingly distinct ways of doing so.\n\nDevelopers reading the Rcpp Attributes documentation will find that package C++ code can be shared by generating a C++ interface for functions that also have an R interface, using Rcpp attributes.\nDevelopers instead scanning widely used Rcpp packages such as {RcppEigen} will notice that C++ code can also be shared by defining the majority of the C++ functions in a package header, to which other Rcpp packages can be linked.\n\nThese are simply different pathways to the writing and export of an R(cpp) package header, which allows Rcpp to link the package’s C++ code to other packages.\nThis blog post explores different ways of doing this, and explains how the Rcpp packages from Epiverse-TRACE implement C++ code sharing."
  },
  {
    "objectID": "posts/share-cpp/index.html#the-package-header",
    "href": "posts/share-cpp/index.html#the-package-header",
    "title": "Sharing the C++ Code of an Rcpp Package",
    "section": "The package header",
    "text": "The package header\nThe package header of the package {mypackage} is a file of the name mypackage.h under inst/include. Defining this header is the key step in making (R)C++ code shareable.\n# conceptual organisation of an Rcpp package with a package header\n.\n├── DESCRIPTION\n├── NAMESPACE\n├── R\n│   └── RcppExports.R\n├── inst\n│   └── include\n│       └── mypackage.h       # &lt;= the package header\n└── src\n    ├── RcppExports.cpp\n    └── rcpp_hello.cpp        # &lt;= code from which RcppExports.cpp generates\n\nAutogenerating the package header\nThe package header is autogenerated when the attributes of an Rcpp function are edited to also generate a C++ interface.\nConsider the Rcpp function below which is exposed to R and exported from the package. The line // [[Rcpp::interfaces(cpp)]] instructs Rcpp to autogenerate two header files under inst/include:\n\nA package header, called mypackage.h, and\nA helper header called mypackage_RcppExports.h with ‘automagic’ C++ bindings for the function hello_world_rcpp().\n\n\n\nsrc/rcpp_hello.cpp\n\n#include &lt;Rcpp.h&gt;\n\n// [[Rcpp::interfaces(cpp)]]\n\n//' @title Test Rcpp function\n//' \n//' @export\n//[[Rcpp::export]]\nvoid hello_world_rcpp() {\n  Rcpp::Rcout &lt;&lt; \"hello world!\\n\";\n}\n\n\n\nManually creating the package header\nThe package header can also be created manually, as mypackage.h under inst/include. In this case, the helper file mypackage_RcppExports.h is not generated.\nExamples of this are the widely used {RcppEigen} and {RcppArmadillo} packages, while this demonstration package by James Balamuta is a minimal example that is a good place to get started to understand how this approach works.\nThe manually defined package header can initially be empty, and is populated by the developer — more on header contents below.\n\n\n\n\n\n\nWarning\n\n\n\nIt is possible to edit an autogenerated package header to include manually created header files in addition to mypackage_RcppExports.h. To do this, remove the generator tag (see below) to prevent this file from being overwritten by Rcpp::compileAttributes(). Then include any extra header files as usual.\nWe would however recommend not autogenerating headers from Rcpp functions, but rather writing a header-heavy package — this is the approach used by {RcppEigen} etc. (see more below on how we organise our packages).\n\n\n\n\nContents of the package header\nWe found it difficult to get information on the content of the package header.\nAutogenerated package headers contain an autogeneration message and a generator token, similar to that present in RcppExports files. Package headers should contain a header include guard.\n\n\n\n\n\n\nTip\n\n\n\nThe style of the header name in the include guard for autogenerated headers is RCPP_mypackage_H_GEN_. Package headers from the Rcpp core team, such as {RcppEigen} and {RcppArmadillo}, are manually defined and follow the convention mypackage__mypackage__h. In examples, such as this bare-bones demonstration package by James Balamuta, you might also encounter a single underscore (_) and a capital H (mypackage_mypackage_H).\nIf you are linting your Rcpp package’s C++ code with Cpplint, all three are incompatible with Cpplint’s preference, which is DIR_SUBDIR_FILE_H. Exclude the package header from linting to avoid this warning if you wish to follow an Rcpp community style instead.\n\n\nThe package header must also link to the code you want to export, and there are at least three ways of doing this.\n\nInclude the autogenerated file mypackage_RcppExports.h; this is already done as part of the package header generation.\nDirectly write C++ code in the package header. This is technically possible, but unlikely to be a good option as your package’s C++ codebase grows.\nManually include any other C++ header files in the package header. This last option might lead to a package header such as that shown below.\n\n\n\ninst/include/mypackage.h\n\n// Manually created package header with manual code inclusion\n#ifndef mypackage_mypackage_H\n#define mypackage_mypackage_H\n\n// include files using paths relative to inst/include\n#include \"header_01.h\"\n#include \"header_02.h\"\n\n#endif  // mypackage_mypackage_H\n\nHere, the header files might contain code that you wish to make available to other packages, such as a C++ function, struct, or class, and indeed in the current package as well — more on how to do this below."
  },
  {
    "objectID": "posts/share-cpp/index.html#using-rcpp-in-header-code",
    "href": "posts/share-cpp/index.html#using-rcpp-in-header-code",
    "title": "Sharing the C++ Code of an Rcpp Package",
    "section": "Using Rcpp in header code",
    "text": "Using Rcpp in header code\nUsing {Rcpp}’s C++ functionality, such as the Rcpp classes DataFrame or List, or classes and functions of Rcpp-based packages such as {RcppEigen}, is as simple as including those headers in the appropriate location, just as one would in a source file — see the example below.\n\n\ninst/include/header_01.h\n\n// In a manually created header file, say, header_01.h\n// which is included in mypackage.h\n\n// to use Rcpp\n#include &lt;Rcpp.h&gt;\n\n// note the use of inline, more on this later\ninline void hello_world_rcpp() {\n  Rcpp::Rcout &lt;&lt; \"hello world!\\n\";\n}\n\nThe appropriate headers are automatically included in autogenerated package headers’ helper files, and the developer need not do anything more.\n\n\n\n\n\n\nTip\n\n\n\nDon’t forget to link to {Rcpp} or similar packages to the package under development by adding the package names under Imports, Depends, or LinkingTo as appropriate.\nThis can often be handled by functions in the {usethis} package such as usethis::use_rcpp_eigen(). You might also need to add // [[Rcpp::depends(&lt;package&gt;)]] in your package’s C++ source files, with a suitable package dependency specified.\n\n\nThe same principles apply to using C++ code from this package ({mypackage}) in future packages."
  },
  {
    "objectID": "posts/share-cpp/index.html#using-header-code-in-the-package",
    "href": "posts/share-cpp/index.html#using-header-code-in-the-package",
    "title": "Sharing the C++ Code of an Rcpp Package",
    "section": "Using header code in the package",
    "text": "Using header code in the package\nThere are some considerations when seeking to use header code from {mypackage} within {mypackage} itself.\nAny functions defined in the package headers must be inline functions (see the example above). This prevents compilation errors related to multiple definitions.\nC++ source files should include the package header, using #include mypackage.h. Functions, structs, or classes defined in header files will be available from the namespace mypackage, as shown in the example below.\nThe code in header files will usually need to be wrapped in (R)C++ code that is exposed to R to make functions from the headers available in R — see the snippet below.\n\n\nmypackage/src/hello_world.cpp\n\n// #include &lt;Rcpp.h&gt;       // include Rcpp if necessary\n#include &lt;mypackage.h&gt;     // include package header\n\n// Function exposed to R\n//' @title Rcpp function wrapping a header function\n//'\n//' @export\n// [[Rcpp::export]]\nvoid print_hello_world() {\n  mypackage::hello_world_rcpp();    // note the namespacing\n}\n\n\n\n\n\n\n\nTip\n\n\n\nRemember to add PKG_CPPFLAGS += -I../inst/include/ to both Makevars and Makevars.win under src/. Furthermore, as noted in the Rcpp attributes documentation, the package will not automatically cause a rebuild when headers are modified — this needs to be done manually."
  },
  {
    "objectID": "posts/share-cpp/index.html#linking-header-code-between-pacakges",
    "href": "posts/share-cpp/index.html#linking-header-code-between-pacakges",
    "title": "Sharing the C++ Code of an Rcpp Package",
    "section": "Linking header code between pacakges",
    "text": "Linking header code between pacakges\nOnce you have developed your package, you can link to its C++ header code in the same way as you would to any other Rcpp-based package.\nConsider the snippet below which shows how to link the C++ code from {mypackage} in a different package called {yourpackage}.\n\n\nyourpackage/src/hello_world.cpp\n\n// [[Rcpp::depends(mypackage)]]   /// specify dependency\n#include &lt;mypackage.h&gt;\n\n// Define and export an Rcpp function\nvoid print_linked_hello() {\n  mypackage::hello_world_rcpp();\n}\n\nBe sure to add LinkingTo: mypackage in the DESCRIPTION of the second package {yourpackage}."
  },
  {
    "objectID": "posts/share-cpp/index.html#c-code-sharing-in-epiverse-trace",
    "href": "posts/share-cpp/index.html#c-code-sharing-in-epiverse-trace",
    "title": "Sharing the C++ Code of an Rcpp Package",
    "section": "C++ code sharing in Epiverse-TRACE",
    "text": "C++ code sharing in Epiverse-TRACE\nIn Epiverse-TRACE, we have structured the {finalsize} and {epidemics} packages to have manually created headers, following the principles laid out above. We follow some additional principles as well.\n\nHeader-heavy packages\n\nOur packages are header-heavy, so that most of the actual code is defined in the headers. The source files are primarily intended to contain wrappers that expose the header code to R (and our users).\n\nNamespaces to organise header code\n\nOur header code is organised into C++ namespaces, which makes it easier to understand where functions are likely to be defined, and what they might be related to. It also makes it possible to include the package headers (and namespaces) that are relevant to users, rather than including the entire codebase.\n\n\nAs an example, functions related to non-pharmaceutical interventions or vaccination regimes from the {epidemics} package can be used in other packages without also including the compartmental epidemic models contained therein."
  },
  {
    "objectID": "posts/share-cpp/index.html#ensuring-the-quality-of-header-code",
    "href": "posts/share-cpp/index.html#ensuring-the-quality-of-header-code",
    "title": "Sharing the C++ Code of an Rcpp Package",
    "section": "Ensuring the quality of header code",
    "text": "Ensuring the quality of header code\nYou can lint and statically check code in a package header using tools for linting C++ code such as Cpplint and Cppcheck. When doing so, it may be important to specify minimum C++ standards, or even the language (C or C++) to avoid linter errors. This is because tools — such as Cppcheck — assume that headers with the extension .h are C headers, which throws errors when encountering C++ features such as the use of namespaces.\nCppcheck’s language and C++ standard can be set using:\ncppcheck --std=c++14 --language=c++ --enable=warning,style --error-exitcode=1 inst/include/*.h\nFurthermore, header code can also be tested independently of the R(cpp) code that eventually wraps it. This can be done using the Catch2 testing framework, which is conveniently available using {testthat} — this is an extensive topic for another post."
  },
  {
    "objectID": "posts/share-cpp/index.html#conclusion",
    "href": "posts/share-cpp/index.html#conclusion",
    "title": "Sharing the C++ Code of an Rcpp Package",
    "section": "Conclusion",
    "text": "Conclusion\nDeveloping an Rcpp-based package with C++ code sharing in mind takes some organisation, or even reorganisation, of the C++ codebase. It is probably a good idea to consider whether your package will implement code that would be of interest to other developers, or to you in related projects. If either of these is true, it may help to structure your package with C++ code sharing in mind from the very beginning of development. This can substantially reduce development overheads and mistakes associated with maintaining multiple copies of the same or similar code in different projects. Fortunately, some great examples of how to do this are among the most-used Rcpp-based packages, providing both a conceptual template to consult for your work, as well as being a demonstration of how beneficial this practice can be in the long run. In Epiverse-TRACE, we intend to continue developing with C++ code sharing as a core principle so that we and other developers can build on our initial work."
  },
  {
    "objectID": "posts/superspreading_v0.1.0/index.html",
    "href": "posts/superspreading_v0.1.0/index.html",
    "title": "superspreading v0.1.0",
    "section": "",
    "text": "We are very excited to announced the release of a new superspreading version v0.1.0. Here is a automatically generated summary of the changes in this version.\nInitial release of superspreading, an R package to estimate individual-level variation in disease transmission and provide summary metrics for superspreading events."
  },
  {
    "objectID": "posts/superspreading_v0.1.0/index.html#new-features",
    "href": "posts/superspreading_v0.1.0/index.html#new-features",
    "title": "superspreading v0.1.0",
    "section": "New features",
    "text": "New features\n\nOffspring distributions, not available in base R, to fit to transmission data.\nFunctions to calculate the probability an infectious disease will cause an epidemic, go extinct or be contained.\nSummary metric functions to calculate proportion of cases that cause a certain proportion of secondary transmission, as well as which proportion of cases are within clusters of a certain size.\nThree vignettes, including: an introduction to the package, estimating individual-level transmission from data, and the effect of superspreading on epidemic risk.\nUnit tests and documentation files.\nContinuous integration workflows for R package checks, rendering the README.md, calculating test coverage, and deploying the pkgdown website."
  },
  {
    "objectID": "posts/superspreading_v0.1.0/index.html#breaking-changes",
    "href": "posts/superspreading_v0.1.0/index.html#breaking-changes",
    "title": "superspreading v0.1.0",
    "section": "Breaking changes",
    "text": "Breaking changes\n\nNone"
  },
  {
    "objectID": "posts/superspreading_v0.1.0/index.html#bug-fixes",
    "href": "posts/superspreading_v0.1.0/index.html#bug-fixes",
    "title": "superspreading v0.1.0",
    "section": "Bug fixes",
    "text": "Bug fixes\n\nNone"
  },
  {
    "objectID": "posts/superspreading_v0.1.0/index.html#deprecated-and-defunct",
    "href": "posts/superspreading_v0.1.0/index.html#deprecated-and-defunct",
    "title": "superspreading v0.1.0",
    "section": "Deprecated and defunct",
    "text": "Deprecated and defunct\n\nNone"
  },
  {
    "objectID": "posts/comm-software-devel/index.html",
    "href": "posts/comm-software-devel/index.html",
    "title": "Communicating development stages of open-source software",
    "section": "",
    "text": "Software is not immediately stable when being developed. It undergoes design changes, changes to the user interface (application programming interface, API), and features get added or removed over time. Software in a open-source setting, in which the code is publicly hosted in a repository (e.g., Github, GitLab, Bitbucket), allows anyone to track developments. It also allows the developer community to easily contribute to the software.\nThere are certain metrics which can be used to convey the stage of development to users or other developers. For example the number of commits, a repository with few commits may indicate that a project is still in an incipient phase and will undergo several breaking changes. However, different software projects become stable at different rates and the number of commits may mean very different things for a repository containing an operating system compared to an R package with minimal functionality. It is therefore important that developers communicate with users and other developers at what stage the project is in, and how stable the code base is.\nSoftware development, and specifically R package development, has several methods to communicate stability. This blog post will discuss two such methods and give examples for each. The first of these is versioning code, which establishes points in development where the code is ready for use; and the second is lifecycle badges, these can be placed at a different levels within software (e.g., package, function, function argument) to convey how a user should interact and use."
  },
  {
    "objectID": "posts/comm-software-devel/index.html#versioning",
    "href": "posts/comm-software-devel/index.html#versioning",
    "title": "Communicating development stages of open-source software",
    "section": "Versioning",
    "text": "Versioning\nVersioning code is not a new concept and has been used for decades1. It has led to version control systems such as git. However, in this post we are interested in versioning to communicate development.\n\nSemantic versioning\nOne such philosophy is semantic versioning (SemVer). This aims to describe the stage of software development by attaching semantics (i.e. meaning) to the format and numbering of versions. The version system works through three numbers, each separated by a dot. The numbers, from left to right, convey major version, minor version and patch version. As an example, 0.5.2, is newer than 0.3.9.\nEmploying semantic versioning in ones code development allows others to determine whether a package has undergone substantial development and testing, and informs to whether it would make a suitable package to use in a script or as a dependency for another package. Semantic versioning also describes the changes made to a package. As explained on their website, incrementing the major version implies a breaking change, a minor increment is a backwards compatible change and lastly patches are mostly applied to bug fixes. This aids users in understanding whether they should continue using a package, whether their package needs updating due to a breaking change or whether they need to install the newest version because a bug was recently fixed.\nExamples of changes that correspond to major, minor or patch updates can be seen in the version release notes (NEWS.md file) of {dplyr} and {ggplot2}.\nIn R there are several packages that work with versioning, and specifically semantic versioning. The {semver} package provides functions for parsing, rendering and comparing versions. There is also the {semverutils} R package which provides similar functionality using R6. The {usethis} package provides handy utility functions for changing the versions of R packages (usethis::use_version() and usethis::use_dev_version()). R also comes with a package_version() function for creating and validating versions.\nOverall semantic versioning provides what they describe as a “formal specification” to facilitate management of package development and the dependencies of that package. It is the most widely-used versioning system and therefore will be understood by a wide variety of users and developers.\nSome of the critique raised for semantic versioning is the difficulty of defining how changes correspond to a version increment. Semantic versioning states only breaking changes warrant major releases, but a vast re-write of a code base may also justify a major version change. Different breaking changes have different magnitudes, therefore a change to a single exported function or a change to every exported function will be communicated in a single, equal, version increment.\n\n\nAlternatives to semantic versioning\nThere are several other versioning frameworks aside from semantic versioning. One common option is calendar versioning (CalVer). The format of CalVer is usually year-month (YY-MM), or year-month-day (YY-MM-DD), depending on the regularity of releases, and allows appending tags (micros or modifiers, e.g. YY-MM.1).\nOther versioning schemes can appear similar to semantic versioning, but do not follow the guidelines around version semantics. In these cases, a bump in the major version may not relate to a breaking change. Additionally, other numbers can be attached to the traditional x.y.z format, such as build numbers. Build number versioning adds an extra number to specify the build (x.y.z.build_number). There are many other variants but covering all versioning systems is outside the scope of this post.\n\n\nVersioning an R package\nThere are some restrictions on valid version numbers for R packages. The official “Writing R Extensions” guide state:\n\nThis is a sequence of at least two (and usually three) non-negative integers separated by single ‘.’ or ‘-’ characters.\n\n\n\nWhy version?\nThe benefits of versioning apply beyond communicating with users and developers. Implementing versioning eases reproducibility by allowing systems to record which version of a language or package was used. In R this can be achieved in several ways, with some popular examples being the {renv} package and docker."
  },
  {
    "objectID": "posts/comm-software-devel/index.html#lifecycle-badges",
    "href": "posts/comm-software-devel/index.html#lifecycle-badges",
    "title": "Communicating development stages of open-source software",
    "section": "Lifecycle badges",
    "text": "Lifecycle badges\nBadges can be pasted onto visible parts of the code, for example a readme document in the root of the repository, to show the development phase and stability. The three badging systems we will discuss in this post are:\n\nRepoStatus\nTidyverse lifecycles\nReconverse lifecyles\n\n\nRepoStatus\nRepoStatus is a language agnostic set of badges which describe the stages of code development and the possible transitions between those stages.\nAs shown in the figure below, there are multiple stages to communicate both unstable and stable software. There are also multiple paths between each stage, recognising the varied routes software development can take.\n\n\n\nRepoStatus badge system. Reused under CC BY-SA 4.0 from repostatus.org\n\n\n\n\nTidyverse\nThe tidyverse approach is broadly similar to RepoStatus. The {lifecycle} R package contains the description of their process. There are four stages:\n\nExperimental\nStable\nSuperseded (previously called retired)\nDeprecated\n\nMost code will go through the experimental phase, as it will likely change its API and the number and order of arguments might change. Once code is not going to drastically change (i.e. no breaking changes), at least from a users point of view, it can be labelled stable. In the tidyverse lifecycle schematic, all experimental code transitions to stable code.\nThe two stages that follow stable are: superseded and deprecated. The former describes a situation in which a new package, a new function or a new argument, depending on the context, has been developed which the developer feels should be used instead of the now superseded code. Superseded code is still developed in the sense that changes to the language or package that may break the function will be fixed as well as bug fixes, but the function will not received ongoing development. The latter, deprecation, is used in cases when the developer thinks that a package or function should not longer be used. This is primarily employed when code is depended on by other software and therefore deleting the code would cause breaks in reverse dependencies. Thus the deprecation warning allows developers of those dependencies time to make the relevant changes.\n\n\n\n{lifecycle} badge system. Reused under MIT license from lifecycles R package\n\n\nOne of the main differences between the tidyverse lifecycles, compared to the others discussed in this posts is their applicability at different levels in the code. The lifecycle badges can be applied at the package-level (e.g., stringr), the function-level (e.g. dplyr::group_trim()) or the argument level (e.g., dplyr::across()).\nUsing {lifecycle} in a package can be setup using usethis::use_lifecycle(). The {lifecycle} package not only provides badges, but also informative deprecation notices which communicate to users that a function is not longer supported since a version release of a package. This offers the user a chance to find an alternative function for future use.\nThe use of deprecation warnings from {lifecycle} leads onto another aspect of tidyverse development: protracted deprecation. There is no fixed rules on how long after a deprecation warning is made to when code should be removed. In the tidyverse, this process is given ample time in order to allow the many developers that utilise tidyverse software to make the necessary changes. Full descriptions of the {lifecycle} package can be found on the website, including the deprecated use of questioning and maturing stages.\n\n\nReconverse\nReconverse provides four stages of software development:\n\nconcept\nexperimental\nmaturing\nstable\n\nA difference between {lifecycle} and reconverse is the explicit connection between semantic versioning and development stage in reconverse. The transitions between experimental, maturing and stable are linked to the versioning less than 0.1.0, less than 1.0.0 and greater than 1.0.0, respectively.\n\n\nDynamic badges\nAll badge frameworks discussed only offer static badges that require developers to manually update as the project moves between phases. This is subject to the maintainers remembering, which can lead to miscommunication about a package’s stage, which may have move on from being experimental, or not been worked on in years but has an active badge.\nDynamics badges, like those offered by https://shields.io/ give a good indication of how recently the project was changed by showing time since last commit, or the number of commits since last release. These too are not perfect but may better track changes and take the burden of badge updates off the project maintainer."
  },
  {
    "objectID": "posts/comm-software-devel/index.html#communicating-development-in-the-epiverse-trace",
    "href": "posts/comm-software-devel/index.html#communicating-development-in-the-epiverse-trace",
    "title": "Communicating development stages of open-source software",
    "section": "Communicating development in the Epiverse-TRACE",
    "text": "Communicating development in the Epiverse-TRACE\nWithin the Epiverse-TRACE initiative we use semantic versioning and badges to convey to the community interacting with our code at which stage of developement each project is in. We do not have fixed rules on which badges to use and a variety of badges can be found across the repositories in the organisation. For example reconverse badges are used for {linelist}, RepoStatus badge is used in {finalsize}, and tidyverse badges are used in {epiparameter}.\nWe take this approach as no lifecycle badging system is perfect, each with benefits and downsides. The badges from {lifecycle} are the most common and thus recognisable in R package development, however may not port well to other languages or be familiar to developers coming to R from other frameworks. RepoStatus has the benefit of not being designed for a single language, and it’s number of badges gives greater acuity to the stage of development for a project. This may be especially useful if a package is newly developed and {lifecycle} would describe it as experimental, but RepoStatus provides granularity as to whether it is a concept package, work in progress (WIP) or started but abandoned.\nThere is some ambiguity in the semantics of the active stage in RepoStatus, which in the definition is “stable, usable state”, but may be misinterpreted as being unstable but actively developed.\nLastly reconverse provides a system akin to {lifecycle} and may be useful for those working in the epidemiology developer space. However, one downside of the reconverse system is there are no clear semantics for a package being deprecated or archived. As with almost all code, at some point development ceases and this stage should be communicated, even if just to say that the package is not being updated inline with developments in the underlying language, in this case R.\nThere are no plans within Epiverse-TRACE to develop a new badging system as the existing systems cover almost all use cases. In the event that the current development stage cannot be adequately communicated with a single badge from one of the frameworks discussed, a combination of badges can be used. For example, early on in a project adding both the experimental badge from {lifecycle} or reconverse and the WIP badge from RepoStatus may more accurately describe the projects develop pace. Alternatively, the stable badge, from either {lifecycle} or reconverse, can be coupled with either active or inactive from RepoStatus to let other developers know if software will be updated with new language features or dependency deprecations.\nOverall, the use of any of the three lifecycle frameworks described here is better than none."
  },
  {
    "objectID": "posts/comm-software-devel/index.html#footnotes",
    "href": "posts/comm-software-devel/index.html#footnotes",
    "title": "Communicating development stages of open-source software",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://en.wikipedia.org/wiki/Version_control↩︎"
  },
  {
    "objectID": "posts/lint-rcpp/index.html",
    "href": "posts/lint-rcpp/index.html",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "",
    "text": "The R package development ecosystem includes packages such as {lintr} and {styler} that can help to check code style, and to format R code.\nHowever, these packages cannot lint or style the C++ code of {Rcpp} packages. This could leave the C++ code of an Rcpp package less clean than the R code, increasing the technical debt already associated with using two languages.\nIn Epiverse-TRACE, we encounter this issue with {finalsize}, and we anticipate the same issue with further epidemic modelling packages that we seek to develop or adapt, such as {fluEvidenceSynthesis}.\nOur use-case is not unique, of course, and other projects could have their own solutions. One such, from which we have borrowed some ideas, is the Apache Arrow project, whose R package also uses a C++ backend (via {cpp11} rather than {Rcpp})."
  },
  {
    "objectID": "posts/lint-rcpp/index.html#use-case",
    "href": "posts/lint-rcpp/index.html#use-case",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "",
    "text": "The R package development ecosystem includes packages such as {lintr} and {styler} that can help to check code style, and to format R code.\nHowever, these packages cannot lint or style the C++ code of {Rcpp} packages. This could leave the C++ code of an Rcpp package less clean than the R code, increasing the technical debt already associated with using two languages.\nIn Epiverse-TRACE, we encounter this issue with {finalsize}, and we anticipate the same issue with further epidemic modelling packages that we seek to develop or adapt, such as {fluEvidenceSynthesis}.\nOur use-case is not unique, of course, and other projects could have their own solutions. One such, from which we have borrowed some ideas, is the Apache Arrow project, whose R package also uses a C++ backend (via {cpp11} rather than {Rcpp})."
  },
  {
    "objectID": "posts/lint-rcpp/index.html#choice-of-c-linters",
    "href": "posts/lint-rcpp/index.html#choice-of-c-linters",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Choice of C++ linters",
    "text": "Choice of C++ linters\nC++ linters such as clang-tidy stumble when dealing with C++ code in src/, as the clang toolchain attempts to compile it. This does not work for Rcpp packages, as the Rcpp.h header cannot be found — this linking is handled by {Rcpp}.\nFortunately, other C++ linters and code checking tools are available and can be used safely with Rcpp packages.\nWe have chosen to use cpplint and cppcheck for {finalsize}.\n\nCpplint\ncpplint is a tool that checks whether C/C++ files follow Google’s C++ style guide. cpplint is easy to install across platforms, and does not error when it cannot find Rcpp.h.\nImportantly, cpplint can be instructed to not lint the autogenerated RcppExports.cpp file, which follows a different style.\nTo lint all other .cpp files, we simply run cpplint from the terminal.\ncpplint --exclude=\"src/RcppExports.cpp\" src/*.cpp\n\n\nCppcheck\ncppcheck is a static code analysis tool, that aims to “have very few false positives”. This is especially useful for the non-standard organisation of Rcpp projects compared to C++ projects.\ncppcheck can also be run locally and instructed to ignore the autogenerated RcppExports.cpp file, while throwing up issues with style.\ncppcheck -i src/RcppExports.cpp --enable=style --error-exitcode=1 src\nHere, the --enable=style option lets cppcheck flag issues with style, acting as a second linter. This enables the performance and portability flags as well. (We have not found any difference when using --enable=warning instead.)\nEnabling all checks (--enable=all) would flag two specific issues for {Rcpp} packages: (1) the Rcpp*.h headers not being found (of the class missingIncludeSystem), and (2) the solver functions not being used by any other C++ function (unusedFunction).\nThese extra options should be avoided in {Rcpp} packages, as the linking is handled for us, and the functions are indeed used later — just not by other C++ functions.\nThe --error-exitcode=1 argument returns the integer 1 when an error is found, which is by convention the output for an error."
  },
  {
    "objectID": "posts/lint-rcpp/index.html#adding-c-linting-to-ci-workflows",
    "href": "posts/lint-rcpp/index.html#adding-c-linting-to-ci-workflows",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Adding C++ linting to CI workflows",
    "text": "Adding C++ linting to CI workflows\nBoth cpplint and cppcheck can be easily added to continuous integration workflows. In Epiverse-TRACE, we use Github Actions. The C++ lint workflow we have implemented looks like this:\non:\n  push:\n    paths: \"src/**\"\n  pull_request:\n    branches:\n      - \"*\"\n\nname: Cpp-lint-check\n\njobs:\n  cpplint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v2\n      - run: pip install cpplint\n      - run: cpplint --quiet --exclude=\"src/RcppExports.cpp\" src/*.cpp\n\n  cppcheck:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt-get install cppcheck\n      - run: cppcheck -i src/RcppExports.cpp --quiet --enable=warning --error-exitcode=1 .\nThe workflow is triggered when there are changes to files in src/, and on all pull requests."
  },
  {
    "objectID": "posts/lint-rcpp/index.html#formatting-c-code",
    "href": "posts/lint-rcpp/index.html#formatting-c-code",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Formatting C++ code",
    "text": "Formatting C++ code\nC++ code can be automatically formatted to avoid linter errors. An especially useful tool is clang-format. Our code is styled to follow the Google C++ style guide using:\n# replace .cpp with .h to format headers\nclang-format -i -style=google src/*.cpp\nHowever, this also formats the autogenerated RcppExports.cpp file. It can be extra work to repeatedly undo this change and keep the original formatting, but clang-format does not provide an easy inline way to ignore this file.\nInstead, clang-format can be passed all files except RcppExports.cpp to style using some simple shell commands. In smaller projects, it might be worth\nfind src -name \"*.cpp\" ! -name \"RcppExports.cpp\" -exec clang-format -style=google -i {} \\;"
  },
  {
    "objectID": "posts/lint-rcpp/index.html#turning-off-linting-and-formatting",
    "href": "posts/lint-rcpp/index.html#turning-off-linting-and-formatting",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Turning off linting and formatting",
    "text": "Turning off linting and formatting\nThere are cases in which we might want to turn linting and formatting off. This might be when the linter does not agree with valid C++ code required in the project, or when the linters and stylers do not agree with each other. These tools are developed separately by large software projects with their own internal requirements, and solutions to issues encountered in their work: clang-format by LLVM (although specifying -style=google), and cpplint from Google’s work.\n\nLinter-enforced paradigms\nSometimes, the linter or styler developer enforces both a style and the use of certain programming paradigms. An example from cpplint is when it warns against passing function arguments by reference, and prefers for these to be passed as pointers, or as constant references (const int &value).\nint some_function(int &value) { \n  /* operations modifying value */\n  return value;\n}\nPassing the argument as a const reference would not serve the needs of this function, and passing by value is a valid strategy when we don’t want to get into the details of using pointers. (Note that this is typically an issue when large objects such as custom classes or structs are passed to a function multiple times.)\nSimilarly, cpplint will throw a warning about accessing variables using std::move, which is something we encounter in the Newton solver in {finalsize}. While not technically wrong for such a simple use case, the linter is correct to cautiously throw a warning nonetheless.\n\n\nLinter-styler disagreement\nOne example of linter-styler disagreement is the use of BOOST_FOREACH from the Boost libraries as an alternative to for loops. clang-format will insist on adding two spaces before the opening bracket: BOOST_FOREACH  (). cpplint will insist on removing one space.\ncpplint and clang-format also disagree on the order of header inclusions, especially when both local and system headers are included.\n\n\nDisabling checks on code chunks\nEither of these cases could require disabling linting or formatting on some part of the code. It is possible to turn off linting using cpplint at particular lines using the comment // NOLINT. Multiple lines can be protected from linting as well.\n// NOLINTBEGIN\n&lt;some C++ code here&gt;\n// NOLINTEND\nAlternatively, clang-format can be instructed to ignore chunks of code using comment messages too.\n// clang-format off\n&lt;some C++ code here&gt;\n// clang-format on"
  },
  {
    "objectID": "posts/lint-rcpp/index.html#linter-options-for-future-packages",
    "href": "posts/lint-rcpp/index.html#linter-options-for-future-packages",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Linter options for future packages",
    "text": "Linter options for future packages\n{finalsize} is a relatively simple {Rcpp} package, with no C/C++ headers, and no C++ tests. However, future Epiverse-TRACE packages could be more similar to {fluEvidenceSynthesis}, and will have header files, and could also have C++ unit tests via the catch framework.\ncpplint will demand that all local headers be prefixed with their directory (src/), but this would cause the code to break as {Rcpp} looks for a subdirectory called src/src/. This can be turned off by passing the filter option --filter=\"-build/include_subdir\" to cpplint. Alternatively, we could place headers in a subdirectory such as inst/include.\nBoth cpplint and cppcheck can be instructed to ignore C++ test files using the catch testing framework provided by {testthat}. This prevents errors due to the specialised syntax provided by {testthat} in testthat.h, such as context.\n# for cpplint, add an extra exclude statement\ncpplint &lt;...&gt; --exclude=\"src/test*.cpp\" src/*.cpp\n\n# for cppcheck, suppress checks on test files\ncppcheck &lt;...&gt; --suppress=*:src/test_*.cpp src"
  },
  {
    "objectID": "posts/lint-rcpp/index.html#conclusion",
    "href": "posts/lint-rcpp/index.html#conclusion",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Conclusion",
    "text": "Conclusion\nIt is actually somewhat surprising that there does not seem to be a canonical linter for C++ code in {Rcpp} packages. The methods laid out here are an initial implementation developed for use with the {finalsize} package, and the considerations here are a starting point. We shall be continuously evaluating how we ensure the quality of our C++ code as we encounter more use cases while developing future Epiverse-TRACE packages."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Epiverse-TRACE developer space",
    "section": "",
    "text": "This is the developer space of the Epiverse-TRACE project, where we share opinions and investigations in R package development, or scientific software development more generally."
  },
  {
    "objectID": "index.html#how-to-collaborate-with-epiverse-on-software-development",
    "href": "index.html#how-to-collaborate-with-epiverse-on-software-development",
    "title": "Epiverse-TRACE developer space",
    "section": "How to collaborate with Epiverse on software development?",
    "text": "How to collaborate with Epiverse on software development?\nEpiverse-TRACE aims to support the development of integrated, generalisable and scalable community-driven software for epidemic analytics, and contribute to a sustainable ecosystem of existing and new tools. There are several ways for external developers and research groups to join us in contributing to these aims. From less to more involvement, you can get involved by following these strategies:\n\nStandardise: by independently developing tools that are consistent with Epiverse principles and interoperable with Epiverse tools. See our blueprint for more.\nConsult us: you can seek help and contributions from Epiverse members to your existing packages/tools, which will also ensure their interoperability with Epiverse’s own packages, e.g. our team currently contributes to the development of GoDataR.\n\nIn the first instance, we encourage you to post general questions on our discussion board so that other community members can contribute.\nFor questions related to a specific package, please contact the maintainer (see the repositories of packages and their authors).\nFor all other questions which fall outside the above categories, please contact Community Manager (anna.carnegie @ lshtm.ac.uk) in the first instance. You can also sign up to our mailing list, or follow us on Twitter at Epiverse_TRACE.\n\nCo-create: develop tools together with Epiverse members, that will then be hosted on independent websites/platforms, such as the WHO collaborative epidemiological parameters initiative.\nIntegrate your tools: Epiverse data analytic pipelines aim to help users optimise their analysis of specific epidemiological problems, e.g. estimating the transmissibility of an infectious disease during an epidemic from line list data. Integration with such pipelines will allow your methods to reach a wider audience and benefit from upstream and downstream tools within Epiverse.\nParticipate: get involved with the Epiverse team and contribute to the development of Epiverse’s existing tools, see our GitHub repository, with your contributions credited, e.g. by looking at the issues raised within Epiverse repositories, adding your own ideas or providing feedback by commenting on our discussion board."
  },
  {
    "objectID": "index.html#cómo-colaborar-en-el-desarrollo-de-software-con-epiverse-trace",
    "href": "index.html#cómo-colaborar-en-el-desarrollo-de-software-con-epiverse-trace",
    "title": "Epiverse-TRACE developer space",
    "section": "¿Cómo colaborar en el desarrollo de software con Epiverse-TRACE?",
    "text": "¿Cómo colaborar en el desarrollo de software con Epiverse-TRACE?\nEl objetivo de Epiverse-TRACE es promover el desarrollo de software para análisis epidemiológico que sea generalizable, escalable y dirigido por la comunidad de usuarios, y que se integre con otras herramientas para crear un ecosistema sostenible. Existen diferentes vías que grupos de investigación y desarrolladores externos pueden utilizar para colaborar con Epiverse, ordenadas de menor a mayor nivel de contacto con el equipo:\n\nEstandarización: desarrollando herramientas independientemente que cumplan con los principios establecidos por Epiverse, y que sean interoperables con sus herramientas.\nConsulta: poniéndose en contacto con miembros del equipo de Epiverse, que pueden contribuir al desarrollo de herramientas externas. Esto garantiza que esas herramientas sean compatibles con paquetes desarrollados por Epiverse, por ejemplo, nuestro equipo actualmente colabora con el desarrollo de GoDataR.\nCo-desarrollo: desarrollando herramientas junto con miembros de Epiverse, que despues se quedarán en plataformas externas, por ejemplo, la iniciativa de la OMS para parámetros epidemiológicos.\n\nIntegración: Epiverse proporciona sistemas para integrar herramientas, o pipelines, que ayudan a los usuarios a optimizar el análisis de problemas epidemiológicos concretos, por ejemplo, para estimar la transmisión de enfermedades. Incorporar tus herramientas a estos sistemas integrados contribuirá a su visibilidad en la comunidad de usuarios de Epiverse.\nParticipación: contribuyendo al desarrollo de herramientas de Epiverse, en las que aparecerás como coautor en nuestra página web. Para ello, contribuye a issues abiertos en los repositorios de Epiverse, o añade sugerencias, ideas y feedback comentando en nuestro panel de discusión.\n\n\n\n\nFigure of contributions, inspired by CSCCE community participation model (Woodley, Lou, & Pratt, Katie. (2020). The CSCCE Community Participation Model – A framework to describe member engagement and information flow in STEM communities. Zenodo.)"
  },
  {
    "objectID": "learn/git-training-01/index.html",
    "href": "learn/git-training-01/index.html",
    "title": "Introductory training on Git and GitHub basics using Rstudio",
    "section": "",
    "text": "If you want to participate in the next version of this training, please complete this expression of interest form.\n\nAll data will be handled confidentially and may only be published in aggregated and anonymised form in reports on the scope of Epiverse-TRACE."
  },
  {
    "objectID": "learn/git-training-01/index.html#how-to-participate",
    "href": "learn/git-training-01/index.html#how-to-participate",
    "title": "Introductory training on Git and GitHub basics using Rstudio",
    "section": "",
    "text": "If you want to participate in the next version of this training, please complete this expression of interest form.\n\nAll data will be handled confidentially and may only be published in aggregated and anonymised form in reports on the scope of Epiverse-TRACE."
  },
  {
    "objectID": "learn/git-training-01/index.html#what-will-you-learn",
    "href": "learn/git-training-01/index.html#what-will-you-learn",
    "title": "Introductory training on Git and GitHub basics using Rstudio",
    "section": "What will you learn?",
    "text": "What will you learn?\n\nExplain why you need to know about Version Control if you work on an Open Science project.\nDescribe the most common verb commands to track changes using Git as a Version Control software within Rstudio.\nCreate a personal online profile to share my tracked changes on the web using GitHub.\nIdentify good practices to describe my tracked changes using Git and collaborate with others using GitHub."
  },
  {
    "objectID": "learn/git-training-01/index.html#who-is-this-training-for",
    "href": "learn/git-training-01/index.html#who-is-this-training-for",
    "title": "Introductory training on Git and GitHub basics using Rstudio",
    "section": "Who is this training for?",
    "text": "Who is this training for?\nThis training is pitched to beginners with basic knowledge of Git who wish to learn more. Three examples of the type of learner who would benefit from this training are included below. The course will be relevant to those from various backgrounds (research degree students, project managers, etc.)\nWe thought about Lucia, Patricia, and Pepe as learner personas in designing the contents of this training:\n\nLucia is a Field epidemiologist. She uses R to clean data and create plots for outbreak response. She wants to communicate her doubts and ideas with package maintainers.\nPatricia is a PhD student in a team with data analysts. She uses R to analyse infectious disease data. She wants to make her research code reusable and open to public.\nPepe is a Project Manager interested in community building. He wants to learn how to provide feedback on contributing good practices to Open Science projects."
  },
  {
    "objectID": "learn/git-training-01/index.html#what-is-not-included-in-this-training",
    "href": "learn/git-training-01/index.html#what-is-not-included-in-this-training",
    "title": "Introductory training on Git and GitHub basics using Rstudio",
    "section": "What is not included in this training?",
    "text": "What is not included in this training?\nTopics that are out of the scope of this training include:\n\nHow to solve conflicts when merging branches.\nHow to revert commits as a safe method to undo changes.\nHow to review pull requests in GitHub.\nHow to use GitHub Actions for continuous integration of software."
  },
  {
    "objectID": "learn/git-training-01/index.html#duration",
    "href": "learn/git-training-01/index.html#duration",
    "title": "Introductory training on Git and GitHub basics using Rstudio",
    "section": "Duration",
    "text": "Duration\nThe duration of this training is of 3.5 hours with breaks of approximately 5 minutes each 60 minutes."
  },
  {
    "objectID": "learn/git-training-01/index.html#program",
    "href": "learn/git-training-01/index.html#program",
    "title": "Introductory training on Git and GitHub basics using Rstudio",
    "section": "Program",
    "text": "Program\n\n\n\n\n\n\n\nTime\nActivity\n\n\n\n\n2:00 - 2:05\nIntroduction\n\n\n15min\nWhat is version control and why should I use it for Open Science projects?\n\n\n5min\nThe general workflow\n\n\n10min\nHow do I set up to start using Git?\n\n\n10min\nWhere does Git store information?\n\n\n30min\nHow do I record changes in Git?\n\n\n5min\nBreak away from the screen\n\n\n20min\nHow do I record notes about my changes and why?\n\n\n30min\nHow can I identify old versions of files? How do I review my changes?\n\n\n10min\nHow can I tell Git to ignore files I don’t want to track?\n\n\n5min\nBreak away from the screen\n\n\n20min\nHow do I share my changes with others on the web?\n\n\n30min\nHow can I use version control to collaborate with others?\n\n\n15min\nWhat do I do when my changes conflict with someone else’s?\n\n\n5:30 - 5:35\nClosing"
  },
  {
    "objectID": "learn/git-training-01/index.html#materials-you-will-need",
    "href": "learn/git-training-01/index.html#materials-you-will-need",
    "title": "Introductory training on Git and GitHub basics using Rstudio",
    "section": "Materials you will need",
    "text": "Materials you will need\nWe will use Git within the Rstudio graphical interface and GitHub. For this you will need to use these instructions:\n\nInstall R and Rstudio https://posit.co/download/rstudio-desktop/\nInstall Git https://git-scm.com/book/en/v2/Getting-Started-Installing-Git\nCreate a GitHub account https://github.com/signup"
  },
  {
    "objectID": "learn/git-training-01/index.html#materials-we-used",
    "href": "learn/git-training-01/index.html#materials-we-used",
    "title": "Introductory training on Git and GitHub basics using Rstudio",
    "section": "Materials we used",
    "text": "Materials we used\n\nSlides that we used in the training with notes expanding the content for instructors\nTemplate for our document with shared notes\nTemplate for our pre-training form\nTemplate for our feedback form at the end of the training\nVideo of the last version of the training"
  },
  {
    "objectID": "learn/git-training-01/index.html#any-questions",
    "href": "learn/git-training-01/index.html#any-questions",
    "title": "Introductory training on Git and GitHub basics using Rstudio",
    "section": "Any questions?",
    "text": "Any questions?\nIf you need any assistance installing the software or have any other questions about the training, please send an email to andree.valle-campos@lshtm.ac.uk"
  },
  {
    "objectID": "learn/git-training-01/index.html#attributions",
    "href": "learn/git-training-01/index.html#attributions",
    "title": "Introductory training on Git and GitHub basics using Rstudio",
    "section": "Attributions",
    "text": "Attributions\n\nThis page format is inspired by Metadocencia.\nThe image of this feed is from Unsplash, provided by Priscilla Du Preez, free to use under the Unsplash License."
  },
  {
    "objectID": "slides/showcase-spring2023/index.html",
    "href": "slides/showcase-spring2023/index.html",
    "title": "Epiverse-TRACE Spring 2023 showcase",
    "section": "",
    "text": "This showcase is part of a regular cycle of online meetings to present tools for epidemiology."
  },
  {
    "objectID": "slides/showcase-spring2023/index.html#sivirep-slides",
    "href": "slides/showcase-spring2023/index.html#sivirep-slides",
    "title": "Epiverse-TRACE Spring 2023 showcase",
    "section": "sivirep slides",
    "text": "sivirep slides"
  },
  {
    "objectID": "slides/showcase-spring2023/index.html#epico-slides",
    "href": "slides/showcase-spring2023/index.html#epico-slides",
    "title": "Epiverse-TRACE Spring 2023 showcase",
    "section": "epiCo slides",
    "text": "epiCo slides"
  },
  {
    "objectID": "slides/showcase-spring2023/index.html#serofoi-slides",
    "href": "slides/showcase-spring2023/index.html#serofoi-slides",
    "title": "Epiverse-TRACE Spring 2023 showcase",
    "section": "serofoi slides",
    "text": "serofoi slides"
  },
  {
    "objectID": "slides/showcase-spring2023/index.html#recording",
    "href": "slides/showcase-spring2023/index.html#recording",
    "title": "Epiverse-TRACE Spring 2023 showcase",
    "section": "Recording",
    "text": "Recording\nGo to the “Watch the recording” button on the event website to watch back this showcase."
  },
  {
    "objectID": "slides/harmonious-ecosystem/index.html",
    "href": "slides/harmonious-ecosystem/index.html",
    "title": "From disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project",
    "section": "",
    "text": "This presentation was given as part of deRSE23 - Conference for Research Software Engineering in Germany, in the Integration vs. Modularity session."
  },
  {
    "objectID": "slides/harmonious-ecosystem/index.html#abstract",
    "href": "slides/harmonious-ecosystem/index.html#abstract",
    "title": "From disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project",
    "section": "Abstract",
    "text": "Abstract\nThere is an increasing trend of packaging and sharing tools in the epidemiology research community. But these tools remain difficult to use and to integrate in a data analysis pipeline. There is a need for a more integrated approach, ensuring that the various tools work well with one another. For example, minimal data wrangling should be needed to transform the output of one tool before passing it to the next tool down the data analysis pipeline. Similarly, various alternatives for a single step of the pipeline should as much as possible use the same inputs and return the same outputs. In this talk, I will present the Epiverse-TRACE project, which collaborates with many R package developers in epidemiology to integrate their tools in a unified universe. Indeed, the unique and challenging feature of Epiverse is that it doesn’t intend to create a unified universe from scratch, but instead aims at updating existing external pieces of software to better work together. This talk will explain how we identify the key parts that should be updated, and how we make these updates with minimal disruption to the individual package developers and established community of users."
  },
  {
    "objectID": "slides/harmonious-ecosystem/index.html#slides",
    "href": "slides/harmonious-ecosystem/index.html#slides",
    "title": "From disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "slides/harmonious-ecosystem/index.html#questions-from-the-audience",
    "href": "slides/harmonious-ecosystem/index.html#questions-from-the-audience",
    "title": "From disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project",
    "section": "Questions from the audience",
    "text": "Questions from the audience\n\nHow do you incentivize developers to collaborate with you?\n\nAs mentioned in the presentation, we argue that collaboration results in a lower maintenance load for each developer by externalising and sharing the maintenance load of common elements.\nWe also think this is the logical continuation of releasing an open-source piece of software. If the goal is to provide a service to the community, then this service is surely greater if we collaborate to make the various pieces interoperable.\nAs an anecdotal piece of evidence, Nick Tierney, the maintainer of the conmat R package, has been very keen to participate in this project and was immediately convinced of the relevance and benefit for users and the ecosystem as a whole.\n\nHow do you ensure the long-term sustainability of this project?\n\nThis is an important question in the domain of open-source and research software but this is not the specific focus of this project and we don’t provide specific solutions, besides conforming to best practices.\nHowever, if anything, our projects should be more sustainable by construction since they result from a collaborative work and include multiple maintainers. This is also encoded in our projects by creating dedicated GitHub organisations, where all developers can participate on an equal footing, thereby also increasing the lottery factor."
  },
  {
    "objectID": "slides/harmonious-ecosystem/index.html#see-also",
    "href": "slides/harmonious-ecosystem/index.html#see-also",
    "title": "From disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project",
    "section": "See also",
    "text": "See also\n\nThe companion post on our blog"
  },
  {
    "objectID": "slides/showcase-winter2023/index.html",
    "href": "slides/showcase-winter2023/index.html",
    "title": "Epiverse-TRACE Winter 2023 showcase",
    "section": "",
    "text": "This showcase is part of a regular cycle of online meetings to present tools for epidemiology."
  },
  {
    "objectID": "slides/showcase-winter2023/index.html#finalsize-slides",
    "href": "slides/showcase-winter2023/index.html#finalsize-slides",
    "title": "Epiverse-TRACE Winter 2023 showcase",
    "section": "finalsize slides",
    "text": "finalsize slides"
  },
  {
    "objectID": "slides/showcase-winter2023/index.html#epiparameter-slides",
    "href": "slides/showcase-winter2023/index.html#epiparameter-slides",
    "title": "Epiverse-TRACE Winter 2023 showcase",
    "section": "epiparameter slides",
    "text": "epiparameter slides"
  },
  {
    "objectID": "slides/showcase-winter2023/index.html#episoap-slides",
    "href": "slides/showcase-winter2023/index.html#episoap-slides",
    "title": "Epiverse-TRACE Winter 2023 showcase",
    "section": "episoap slides",
    "text": "episoap slides"
  },
  {
    "objectID": "slides/showcase-winter2023/index.html#recording",
    "href": "slides/showcase-winter2023/index.html#recording",
    "title": "Epiverse-TRACE Winter 2023 showcase",
    "section": "Recording",
    "text": "Recording\nGo to the “Watch the recording” button on the event website to watch back this showcase."
  },
  {
    "objectID": "slides/juniper-may-2023/index.html",
    "href": "slides/juniper-may-2023/index.html",
    "title": "Juniper seminar on improving software for epidemic response",
    "section": "",
    "text": "Talk at the Juniper seminar series that included a large section on our efforts to improve the sustainability and interoperability of outbreaks analysis tools, covering lessons from the real-time response to COVID-19, as well as recent work in Epiverse-TRACE."
  },
  {
    "objectID": "slides/juniper-may-2023/index.html#talk-slides",
    "href": "slides/juniper-may-2023/index.html#talk-slides",
    "title": "Juniper seminar on improving software for epidemic response",
    "section": "Talk slides",
    "text": "Talk slides"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Juniper seminar on improving software for epidemic response\n\n\n\n\n\n\n\nepiparameter\n\n\nfinalsize\n\n\nepisoap\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nAdam Kucharski\n\n\n\n\n\n\n  \n\n\n\n\nEpiverse-TRACE Spring 2023 showcase\n\n\n\n\n\n\n\nsivirep\n\n\nepiCo\n\n\nserofoi\n\n\nshowcase\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2023\n\n\nAnna Carnegie, Geraldine Gómez Millán, Juan Daniel Umaña, Nicolás Torres\n\n\n\n\n\n\n  \n\n\n\n\nFrom disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project\n\n\n\n\n\n\n\ndata science\n\n\npipelines\n\n\ninteroperability\n\n\ncommunity\n\n\ndeRSE23\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nHugo Gruson\n\n\n\n\n\n\n  \n\n\n\n\nEpiverse-TRACE Winter 2023 showcase\n\n\n\n\n\n\n\nepiparameter\n\n\nfinalsize\n\n\nepisoap\n\n\nshowcase\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nAnna Carnegie, Pratik Gupte, Joshua Lambert, Hugo Gruson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/extend-dataframes/index.html",
    "href": "posts/extend-dataframes/index.html",
    "title": "Extending Data Frames",
    "section": "",
    "text": "R is a commonly used language for data science and statistical computing. Foundational to this is having data structures that allow manipulation of data with minimal effort and cognitive load. One of the most commonly required data structures is tabular data. This can be represented in R in a few ways, for example a matrix or a data frame. The data frame (class data.frame) is a flexible tabular data structure, as it can hold different data types (e.g. numbers, character strings, etc.) across different columns. This is in contrast to matrices – which are arrays with dimensions – and thus can only hold a single data type.\n\n# data frame can hold heterogeneous data types across different columns\ndata.frame(a = c(1, 2, 3), b = c(4, 5, 6), c = c(\"a\", \"b\", \"c\"))\n\n  a b c\n1 1 4 a\n2 2 5 b\n3 3 6 c\n\n# each column must be of the same type\ndf &lt;- data.frame(a = c(1, 2, 3), b = c(\"4\", 5, 6))\n# be careful of the silent type conversion\ndf$a\n\n[1] 1 2 3\n\ndf$b\n\n[1] \"4\" \"5\" \"6\"\n\nmat &lt;- matrix(1:9, nrow = 3, ncol = 3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\nmat[1, 1] &lt;- \"1\"\n# be careful of the silent type conversion\nmat\n\n     [,1] [,2] [,3]\n[1,] \"1\"  \"4\"  \"7\" \n[2,] \"2\"  \"5\"  \"8\" \n[3,] \"3\"  \"6\"  \"9\" \n\n\nData frames can even be nested, cells can be data frames or lists.\n\ndf &lt;- data.frame(a = \"w\", b = \"x\")\ndf[1, 1][[1]] &lt;- list(c = c(\"y\", \"z\"))\ndf\n\n     a b\n1 y, z x\n\ndf &lt;- data.frame(a = \"w\", b = \"x\")\ndf[1, 1][[1]] &lt;- list(data.frame(c = \"y\", d = \"z\"))\ndf\n\n     a b\n1 y, z x\n\n\nIt is therefore clear why data frames are so prevalent. However, they are not without limitations. They have a relatively basic printing method which can fload the R console when the number of columns or rows is large. They have useful methods (e.g., summary() and str()), but these might not be appropriate for certain types of tabular data. In these cases it is useful to utilise R’s inheritance mechanisms (specifically S3 inheritance) to write extensions for R’s data.frame class. In this case the data frame is the superclass and the new subclass extends it and inherits its methods (see the Advanced R book for more details on S3 inheritance).\nOne of the most common extension of the data frame is the tibble from the {tibble} R package. Outlined in {tibble}’s vignette, tibbles offer improvements in printing, subsetting and recycling rules. Another commonly used data frame extension is the data.table class from the {data.table} R package. In addition to the improved printing, this class is designed to improve the performance (i.e. speed and efficiency of operations and storage) of working with tabular data in R and provide a terse syntax for manipulation.\nIn the process of developing R software (most likely an R package), a new tabular data class that builds atop data frames can become beneficial. This blog post has two main sections:\n\na brief overview of the steps required to setup a class that extends data frames\nguide to the technical aspects of class invariants (required data members of a class) and design and implementation decisions, and tidyverse compatibility\n\n\n\nIt is useful to write a class constructor function that can be called to create an object of your new class. The functions defined below are a redacted version (for readability) of functions available in the {ExtendDataFrames} R package, which contains example functions and files discussed in this post. When assigning the class name ensure that it is a vector containing \"data.frame\" as the last element to correctly inherit properties and methods from the data.frame class.\nbirthdays &lt;- function(x) {\n  # the vector of classes is required for it to inherit from `data.frame`\n  structure(x, class = c(\"birthdays\", \"data.frame\"))\n}\nThat’s all that’s needed to create a subclass of a data frame. However, although we’ve created the class we haven’t given it any functionality and thus it will be identical to a data frame due to inheritance.\nWe can now write as many methods as we want. Here we will show two methods, one of which does not require writing a generic (print.birthdays) and the second that does (birthdays_per_month). The print() generic function is provided by R, which is why we do not need to add one ourselves. See Adv R and this Epiverse blog post to find out more about S3 generics.\nprint.birthdays &lt;- function(x, ...) {\n  cat(\n    sprintf(\n      \"A `birthdays` object with %s rows and %s cols\",\n      dim(x)[1], dim(x)[2]\n    )\n  )\n  invisible(x)\n}\n\nbirthdays_per_month &lt;- function(x, ...) {\n  UseMethod(\"birthdays_per_month\")\n}\n\nbirthdays_per_month.birthdays &lt;- function(x, ...) {\n  out &lt;- table(lubridate::month(x$birthday))\n  months &lt;- c(\n    \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n    \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n  )\n  names(out) &lt;- months[as.numeric(names(out))]\n  return(out)\n}\n\n\n\n\n\n\nTip\n\n\n\nUseful resources for the “Writing custom data class” section: extending tibbles and their functionality\n\n\n\n\n\nWe will now move on to the second section of the post, in which we discuss the design choices when creating and using S3 classes in R. Class invariants are members of your class that define it. In other words, without these elements your class does not fulfil its basic definition. It is therefore sensible to make sure that your class contains these elements at all times (or at least after operations have been applied to your class). In cases when the class object contains all the invariants normal service can be continued. However, in the case that an invariant is missing or modified to a non-conformist type (e.g. a date converted to a numeric) a decision has to be made. Either the code can error, hopefully giving the user an informative message as to why their modification broke the object; alternatively, the subclass can be revoked and the superclass can be returned. In almost all cases the superclass (i.e. the base class being inherited from) is more general and won’t have the same class invariant restrictions.\nFor our example class, &lt;birthdays&gt;, the invariants are a column called name which must contain characters, and a column called birthday which must contain dates. The order of the rows and columns is not considered an invariant property, and having extra columns with other names and data types is also allowed. The number of rows is also not an invariant as we can have as many birthdays as we like in the data object.\nHere we present both cases as well as considerations and technical details of both options. We’ll demonstrate both of these cases with the subset function in R (subsetting uses a single square bracket for tabular data, [). First the fail-on-subsetting. Before we write the subsetting function it is useful to have a function that checks that an object of our class is valid, a so-called validator function.\n\nvalidate_birthdays &lt;- function(x) {\n  stopifnot(\n    \"input must contain 'name' and 'birthday' columns\" =\n      all(c(\"name\", \"birthday\") %in% colnames(x)),\n    \"names must be a character\" =\n      is.character(x$name),\n    \"birthday must be a date\" =\n      lubridate::is.Date(x$birthday)\n  )\n  invisible(x)\n}\n\nThis will return an error if the class is not valid (defined in terms of the class’ invariants).\nNow we can show how to error if one of the invariants are removed during subsetting. See ?NextMethod() for information on method dispatch.\n\n`[.birthdays` &lt;- function(x) {\n  validate_birthdays(NextMethod())\n}\n\nbirthdays[, -1]\n#  Error in validate_birthdays(NextMethod()) :\n#  input must contain 'name' and 'birthday' columns\n\nThe second design option is the reconstruct-on-subsetting. This checks whether the class is valid, and if not downgrade the class to the superclass, in our case a data frame. This is done by not only validating the object during subsetting but to check whether it is a valid class object, and then either ensuring all of the attributes of the subclass – in our case &lt;birthdays&gt; – are maintained, or attributes are stripped and only the attributes of the base superclass – in our case data.frame – are kept.\n\n\n\n\n\n\nNote\n\n\n\nImportant note: this section of the post relies heavily on https://github.com/DavisVaughan/2020-06-01_dplyr-vctrs-compat.\n\n\nThe four functions that are required to be added to ensure our class is correctly handled when invaliding it are:\n\nbirthdays_reconstruct()\nbirthdays_can_reconstruct()\ndf_reconstruct()\ndplyr_reconstruct.birthdays()\n\nWe’ll tackle the first three first, and then move onto to the last one as this requires some extra steps.\nbirthdays_reconstruct() is a function that contains an if-else statement to determine whether the returned object is a &lt;birthdays&gt; or data.frame object.\n\nbirthdays_reconstruct &lt;- function(x, to) {\n  if (birthdays_can_reconstruct(x)) {\n    df_reconstruct(x, to)\n  } else {\n     x &lt;- as.data.frame(x)\n    message(\"Removing crucial column in `&lt;birthdays&gt;` returning `&lt;data.frame&gt;`\")\n    x\n  }\n}\n\nThe if-else evaluation is controlled by birthdays_can_reconstruct(). This function determines whether after subsetting the object is a valid &lt;birthdays&gt; class. It checks whether the validator fails, in which case it returns FALSE, otherwise the function will return TRUE.\n\nbirthdays_can_reconstruct &lt;- function(x) {\n  # check whether input is valid\n  valid &lt;- tryCatch(\n    { validate_birthdays(x) },\n    error = function(cnd) FALSE\n  )\n\n  # return boolean\n  !isFALSE(valid)\n}\n\nThe next function required is df_reconstruct(). This is called when the object is judged to be a valid &lt;birthdays&gt; object and simply copies the attributes over from the &lt;birthdays&gt; class to the object being subset.\n\ndf_reconstruct &lt;- function(x, to) {\n  attrs &lt;- attributes(to)\n  attrs$names &lt;- names(x)\n  attrs$row.names &lt;- .row_names_info(x, type = 0L)\n  attributes(x) &lt;- attrs\n  x\n}\n\nThe three functions defined for reconstruction can be added to a package with the subsetting function in order to subset &lt;birthdays&gt; objects and returning either &lt;birthdays&gt; objects if still valid, or data frames when invalidated. This design has the benefit that when conducting data exploration a user is not faced with an error, but can continue with a data frame, while being informed by the message printed to console in birthdays_reconstruct().\n\n`[.birthdays` &lt;- function(x, ...) {\n  out &lt;- NextMethod()\n  birthdays_reconstruct(out, x)\n}\n\n\n\n\n\nlibrary(dplyr)\n\nIn order to be able to operate on our &lt;birthdays&gt; class using functions from the package {dplyr}, as would be common for data frames, we need to make our function compatible. This is where the function dplyr_reconstruct.birthdays() comes in. dplyr_reconstruct() is a generic function exported by {dplyr}. It is called in {dplyr} verbs to make sure that the objects are restored to the input class when not invalidated.\n\ndplyr_reconstruct.birthdays &lt;- function(data, template) { # nolint\n  birthdays_reconstruct(data, template)\n}\n\nInformation about the generic can be found through the {dplyr} help documentation.\n\n?dplyr::dplyr_extending\n?dplyr::dplyr_reconstruct\n\nAs explained in the help documentation, {dplyr} also uses two base R functions to perform data manipulation. names&lt;- (i.e the names setter function) and [ the one-dimensional subsetting function. We therefore define these methods for our custom class in order for dplyr_reconstruct() to work as intended.\n\n`[.birthdays` &lt;- function(x, ...) {\n  out &lt;- NextMethod()\n  birthdays_reconstruct(out, x)\n}\n\n`names&lt;-.birthdays` &lt;- function(x, value) {\n  out &lt;- NextMethod()\n  birthdays_reconstruct(out, x)\n}\n\nThis wraps up the need for adding function to perform data manipulation using the reconstruction design outlined above.\nHowever, there is some final housekeeping to do. In cases when {dplyr} is not a package dependency (either imported or suggested), then the S3 generic dplyr_reconstruct() is required to be loaded. In R versions before 3.6.0 – this also works for R versions later than 3.6.0 – the generic function needs to be registered. This is done by writing an .onLoad() function, typically in a file called zzz.R. This is included in the {ExtendDataFrames} package for illustrative purposes.\n\n\n\nzzz.R\n\n.onLoad &lt;- function(libname, pkgname) {\n  s3_register(\"dplyr::dplyr_reconstruct\", \"birthdays\")\n  invisible()\n}\n\n\nThe s3_register() function used in .onLoad() also needs to be added to the package and this function is kindly supplied by both {vctrs} and {rlang} unlicensed and thus can be copied into another package. See the R packages book for information about .onLoad() and attaching and loading in general.\nSince R version 3.6.0 this S3 generic registration happens automatically with S3Method() in the package namespace using the {roxygen2} documentation #' @exportS3Method dplyr::dplyr_reconstruct.\nThere is one last option which prevents the hard dependency on a relatively recent R version. Since {roxygen2} version 6.1.0, there is the @rawNamespace tag which allows insertion of text into the NAMESPACE file. Using this tag the following code will check the local R version and register the S3 method if equal to or above 3.6.0.\n\n#' @rawNamespace if (getRversion() &gt;= \"3.6.0\") {\n#'   S3method(pkg::fun, class)\n#' }\n\nEach of the three options for registering S3 methods has different benefits and downsides, so the choice depends on the specific use-case. Over time it may be best to use the most up-to-date methods as packages are usually only maintained for a handful of recent R releases1.\nThe topics discussed in this post have been implemented in the {epiparameter} R package within Epiverse-TRACE.\nCompatibility with {vctrs} is also possible using the same mechanism (functions) described in this post, and if interested see https://github.com/DavisVaughan/2020-06-01_dplyr-vctrs-compat for details.\nFor other use-cases and discussions of the designs and implementations discussed in this post see:\n\n{dials} R package\n{rsample} R package\n{googledrive} R package\nPull request on {tibble} R package\n\nThis blog post is a compendium of information from sources that are linked and cited throughout. Please refer to those sites for more information and as the primary source for citation in further work."
  },
  {
    "objectID": "posts/extend-dataframes/index.html#extending-data-frames-in-r",
    "href": "posts/extend-dataframes/index.html#extending-data-frames-in-r",
    "title": "Extending Data Frames",
    "section": "",
    "text": "R is a commonly used language for data science and statistical computing. Foundational to this is having data structures that allow manipulation of data with minimal effort and cognitive load. One of the most commonly required data structures is tabular data. This can be represented in R in a few ways, for example a matrix or a data frame. The data frame (class data.frame) is a flexible tabular data structure, as it can hold different data types (e.g. numbers, character strings, etc.) across different columns. This is in contrast to matrices – which are arrays with dimensions – and thus can only hold a single data type.\n\n# data frame can hold heterogeneous data types across different columns\ndata.frame(a = c(1, 2, 3), b = c(4, 5, 6), c = c(\"a\", \"b\", \"c\"))\n\n  a b c\n1 1 4 a\n2 2 5 b\n3 3 6 c\n\n# each column must be of the same type\ndf &lt;- data.frame(a = c(1, 2, 3), b = c(\"4\", 5, 6))\n# be careful of the silent type conversion\ndf$a\n\n[1] 1 2 3\n\ndf$b\n\n[1] \"4\" \"5\" \"6\"\n\nmat &lt;- matrix(1:9, nrow = 3, ncol = 3)\nmat\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\nmat[1, 1] &lt;- \"1\"\n# be careful of the silent type conversion\nmat\n\n     [,1] [,2] [,3]\n[1,] \"1\"  \"4\"  \"7\" \n[2,] \"2\"  \"5\"  \"8\" \n[3,] \"3\"  \"6\"  \"9\" \n\n\nData frames can even be nested, cells can be data frames or lists.\n\ndf &lt;- data.frame(a = \"w\", b = \"x\")\ndf[1, 1][[1]] &lt;- list(c = c(\"y\", \"z\"))\ndf\n\n     a b\n1 y, z x\n\ndf &lt;- data.frame(a = \"w\", b = \"x\")\ndf[1, 1][[1]] &lt;- list(data.frame(c = \"y\", d = \"z\"))\ndf\n\n     a b\n1 y, z x\n\n\nIt is therefore clear why data frames are so prevalent. However, they are not without limitations. They have a relatively basic printing method which can fload the R console when the number of columns or rows is large. They have useful methods (e.g., summary() and str()), but these might not be appropriate for certain types of tabular data. In these cases it is useful to utilise R’s inheritance mechanisms (specifically S3 inheritance) to write extensions for R’s data.frame class. In this case the data frame is the superclass and the new subclass extends it and inherits its methods (see the Advanced R book for more details on S3 inheritance).\nOne of the most common extension of the data frame is the tibble from the {tibble} R package. Outlined in {tibble}’s vignette, tibbles offer improvements in printing, subsetting and recycling rules. Another commonly used data frame extension is the data.table class from the {data.table} R package. In addition to the improved printing, this class is designed to improve the performance (i.e. speed and efficiency of operations and storage) of working with tabular data in R and provide a terse syntax for manipulation.\nIn the process of developing R software (most likely an R package), a new tabular data class that builds atop data frames can become beneficial. This blog post has two main sections:\n\na brief overview of the steps required to setup a class that extends data frames\nguide to the technical aspects of class invariants (required data members of a class) and design and implementation decisions, and tidyverse compatibility\n\n\n\nIt is useful to write a class constructor function that can be called to create an object of your new class. The functions defined below are a redacted version (for readability) of functions available in the {ExtendDataFrames} R package, which contains example functions and files discussed in this post. When assigning the class name ensure that it is a vector containing \"data.frame\" as the last element to correctly inherit properties and methods from the data.frame class.\nbirthdays &lt;- function(x) {\n  # the vector of classes is required for it to inherit from `data.frame`\n  structure(x, class = c(\"birthdays\", \"data.frame\"))\n}\nThat’s all that’s needed to create a subclass of a data frame. However, although we’ve created the class we haven’t given it any functionality and thus it will be identical to a data frame due to inheritance.\nWe can now write as many methods as we want. Here we will show two methods, one of which does not require writing a generic (print.birthdays) and the second that does (birthdays_per_month). The print() generic function is provided by R, which is why we do not need to add one ourselves. See Adv R and this Epiverse blog post to find out more about S3 generics.\nprint.birthdays &lt;- function(x, ...) {\n  cat(\n    sprintf(\n      \"A `birthdays` object with %s rows and %s cols\",\n      dim(x)[1], dim(x)[2]\n    )\n  )\n  invisible(x)\n}\n\nbirthdays_per_month &lt;- function(x, ...) {\n  UseMethod(\"birthdays_per_month\")\n}\n\nbirthdays_per_month.birthdays &lt;- function(x, ...) {\n  out &lt;- table(lubridate::month(x$birthday))\n  months &lt;- c(\n    \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n    \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n  )\n  names(out) &lt;- months[as.numeric(names(out))]\n  return(out)\n}\n\n\n\n\n\n\nTip\n\n\n\nUseful resources for the “Writing custom data class” section: extending tibbles and their functionality\n\n\n\n\n\nWe will now move on to the second section of the post, in which we discuss the design choices when creating and using S3 classes in R. Class invariants are members of your class that define it. In other words, without these elements your class does not fulfil its basic definition. It is therefore sensible to make sure that your class contains these elements at all times (or at least after operations have been applied to your class). In cases when the class object contains all the invariants normal service can be continued. However, in the case that an invariant is missing or modified to a non-conformist type (e.g. a date converted to a numeric) a decision has to be made. Either the code can error, hopefully giving the user an informative message as to why their modification broke the object; alternatively, the subclass can be revoked and the superclass can be returned. In almost all cases the superclass (i.e. the base class being inherited from) is more general and won’t have the same class invariant restrictions.\nFor our example class, &lt;birthdays&gt;, the invariants are a column called name which must contain characters, and a column called birthday which must contain dates. The order of the rows and columns is not considered an invariant property, and having extra columns with other names and data types is also allowed. The number of rows is also not an invariant as we can have as many birthdays as we like in the data object.\nHere we present both cases as well as considerations and technical details of both options. We’ll demonstrate both of these cases with the subset function in R (subsetting uses a single square bracket for tabular data, [). First the fail-on-subsetting. Before we write the subsetting function it is useful to have a function that checks that an object of our class is valid, a so-called validator function.\n\nvalidate_birthdays &lt;- function(x) {\n  stopifnot(\n    \"input must contain 'name' and 'birthday' columns\" =\n      all(c(\"name\", \"birthday\") %in% colnames(x)),\n    \"names must be a character\" =\n      is.character(x$name),\n    \"birthday must be a date\" =\n      lubridate::is.Date(x$birthday)\n  )\n  invisible(x)\n}\n\nThis will return an error if the class is not valid (defined in terms of the class’ invariants).\nNow we can show how to error if one of the invariants are removed during subsetting. See ?NextMethod() for information on method dispatch.\n\n`[.birthdays` &lt;- function(x) {\n  validate_birthdays(NextMethod())\n}\n\nbirthdays[, -1]\n#  Error in validate_birthdays(NextMethod()) :\n#  input must contain 'name' and 'birthday' columns\n\nThe second design option is the reconstruct-on-subsetting. This checks whether the class is valid, and if not downgrade the class to the superclass, in our case a data frame. This is done by not only validating the object during subsetting but to check whether it is a valid class object, and then either ensuring all of the attributes of the subclass – in our case &lt;birthdays&gt; – are maintained, or attributes are stripped and only the attributes of the base superclass – in our case data.frame – are kept.\n\n\n\n\n\n\nNote\n\n\n\nImportant note: this section of the post relies heavily on https://github.com/DavisVaughan/2020-06-01_dplyr-vctrs-compat.\n\n\nThe four functions that are required to be added to ensure our class is correctly handled when invaliding it are:\n\nbirthdays_reconstruct()\nbirthdays_can_reconstruct()\ndf_reconstruct()\ndplyr_reconstruct.birthdays()\n\nWe’ll tackle the first three first, and then move onto to the last one as this requires some extra steps.\nbirthdays_reconstruct() is a function that contains an if-else statement to determine whether the returned object is a &lt;birthdays&gt; or data.frame object.\n\nbirthdays_reconstruct &lt;- function(x, to) {\n  if (birthdays_can_reconstruct(x)) {\n    df_reconstruct(x, to)\n  } else {\n     x &lt;- as.data.frame(x)\n    message(\"Removing crucial column in `&lt;birthdays&gt;` returning `&lt;data.frame&gt;`\")\n    x\n  }\n}\n\nThe if-else evaluation is controlled by birthdays_can_reconstruct(). This function determines whether after subsetting the object is a valid &lt;birthdays&gt; class. It checks whether the validator fails, in which case it returns FALSE, otherwise the function will return TRUE.\n\nbirthdays_can_reconstruct &lt;- function(x) {\n  # check whether input is valid\n  valid &lt;- tryCatch(\n    { validate_birthdays(x) },\n    error = function(cnd) FALSE\n  )\n\n  # return boolean\n  !isFALSE(valid)\n}\n\nThe next function required is df_reconstruct(). This is called when the object is judged to be a valid &lt;birthdays&gt; object and simply copies the attributes over from the &lt;birthdays&gt; class to the object being subset.\n\ndf_reconstruct &lt;- function(x, to) {\n  attrs &lt;- attributes(to)\n  attrs$names &lt;- names(x)\n  attrs$row.names &lt;- .row_names_info(x, type = 0L)\n  attributes(x) &lt;- attrs\n  x\n}\n\nThe three functions defined for reconstruction can be added to a package with the subsetting function in order to subset &lt;birthdays&gt; objects and returning either &lt;birthdays&gt; objects if still valid, or data frames when invalidated. This design has the benefit that when conducting data exploration a user is not faced with an error, but can continue with a data frame, while being informed by the message printed to console in birthdays_reconstruct().\n\n`[.birthdays` &lt;- function(x, ...) {\n  out &lt;- NextMethod()\n  birthdays_reconstruct(out, x)\n}\n\n\n\n\n\nlibrary(dplyr)\n\nIn order to be able to operate on our &lt;birthdays&gt; class using functions from the package {dplyr}, as would be common for data frames, we need to make our function compatible. This is where the function dplyr_reconstruct.birthdays() comes in. dplyr_reconstruct() is a generic function exported by {dplyr}. It is called in {dplyr} verbs to make sure that the objects are restored to the input class when not invalidated.\n\ndplyr_reconstruct.birthdays &lt;- function(data, template) { # nolint\n  birthdays_reconstruct(data, template)\n}\n\nInformation about the generic can be found through the {dplyr} help documentation.\n\n?dplyr::dplyr_extending\n?dplyr::dplyr_reconstruct\n\nAs explained in the help documentation, {dplyr} also uses two base R functions to perform data manipulation. names&lt;- (i.e the names setter function) and [ the one-dimensional subsetting function. We therefore define these methods for our custom class in order for dplyr_reconstruct() to work as intended.\n\n`[.birthdays` &lt;- function(x, ...) {\n  out &lt;- NextMethod()\n  birthdays_reconstruct(out, x)\n}\n\n`names&lt;-.birthdays` &lt;- function(x, value) {\n  out &lt;- NextMethod()\n  birthdays_reconstruct(out, x)\n}\n\nThis wraps up the need for adding function to perform data manipulation using the reconstruction design outlined above.\nHowever, there is some final housekeeping to do. In cases when {dplyr} is not a package dependency (either imported or suggested), then the S3 generic dplyr_reconstruct() is required to be loaded. In R versions before 3.6.0 – this also works for R versions later than 3.6.0 – the generic function needs to be registered. This is done by writing an .onLoad() function, typically in a file called zzz.R. This is included in the {ExtendDataFrames} package for illustrative purposes.\n\n\n\nzzz.R\n\n.onLoad &lt;- function(libname, pkgname) {\n  s3_register(\"dplyr::dplyr_reconstruct\", \"birthdays\")\n  invisible()\n}\n\n\nThe s3_register() function used in .onLoad() also needs to be added to the package and this function is kindly supplied by both {vctrs} and {rlang} unlicensed and thus can be copied into another package. See the R packages book for information about .onLoad() and attaching and loading in general.\nSince R version 3.6.0 this S3 generic registration happens automatically with S3Method() in the package namespace using the {roxygen2} documentation #' @exportS3Method dplyr::dplyr_reconstruct.\nThere is one last option which prevents the hard dependency on a relatively recent R version. Since {roxygen2} version 6.1.0, there is the @rawNamespace tag which allows insertion of text into the NAMESPACE file. Using this tag the following code will check the local R version and register the S3 method if equal to or above 3.6.0.\n\n#' @rawNamespace if (getRversion() &gt;= \"3.6.0\") {\n#'   S3method(pkg::fun, class)\n#' }\n\nEach of the three options for registering S3 methods has different benefits and downsides, so the choice depends on the specific use-case. Over time it may be best to use the most up-to-date methods as packages are usually only maintained for a handful of recent R releases1.\nThe topics discussed in this post have been implemented in the {epiparameter} R package within Epiverse-TRACE.\nCompatibility with {vctrs} is also possible using the same mechanism (functions) described in this post, and if interested see https://github.com/DavisVaughan/2020-06-01_dplyr-vctrs-compat for details.\nFor other use-cases and discussions of the designs and implementations discussed in this post see:\n\n{dials} R package\n{rsample} R package\n{googledrive} R package\nPull request on {tibble} R package\n\nThis blog post is a compendium of information from sources that are linked and cited throughout. Please refer to those sites for more information and as the primary source for citation in further work."
  },
  {
    "objectID": "posts/extend-dataframes/index.html#footnotes",
    "href": "posts/extend-dataframes/index.html#footnotes",
    "title": "Extending Data Frames",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the working practise of tidyverse packages: https://www.tidyverse.org/blog/2019/04/r-version-support/↩︎"
  },
  {
    "objectID": "posts/100days-workshop/index.html",
    "href": "posts/100days-workshop/index.html",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "",
    "text": "This vignette summarises the findings from the 100 days and 100 lines of code workshop, hosted in December 2022 by Epiverse-TRACE.\nTo discuss how the first 100 lines of code during an epidemic should look like, we invited 40 experts, including academics, field epidemiologists, and software engineers, to take part in a 3-day workshop, where they discussed the current challenges, and potential solutions, in data analytic pipelines used to analyse epidemic data. In addition to highlighting existing technical solutions and their use cases, presentations on best practices in fostering collaboration across institutions and disciplines set the scene for the subsequent workshop scenario exercises."
  },
  {
    "objectID": "posts/100days-workshop/index.html#scenario-1-novel-respiratory-disease-in-the-gambia",
    "href": "posts/100days-workshop/index.html#scenario-1-novel-respiratory-disease-in-the-gambia",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "Scenario 1: Novel respiratory disease in The Gambia",
    "text": "Scenario 1: Novel respiratory disease in The Gambia\n\n\n\nScenario 1 details\n\n\n\nAnalytic pipeline for scenario 1 (analysed by group 2)\n\nData cleaning\n\nlinelist to standardise date format\ncleanr from previous Hackathon\n\nDelay distributions\n\nfitdisrplus to fit parameteric distributions to scenario data\nepiparameter to extract delay distributions from respiratory pathogens\nEpiNow2 to fit reporting delays\nEpiEstim / coarseDataTools to estimate generation time/serial interval of disease\nepicontacts\nmixdiff to estimate delay distributions and correct erroneous dates at the same time (still under development)\n\nPopulation demographics\n\nWould like to have had access to an R package similar to ColOpenData\n\nRisk factors of infection\n\nUsed R4epis as a guide on how to create two-way tables and perform Chi-squared tests\n\nSeverity of disease\n\ndatadelay for CFR calculation\nImplementation of method developed by AC Ghani, 2005 to estimate CFR\n\nContact matching\n\ndiyar to match and link records\nfuzzyjoin to join contact and case data despite misspellings or missing cell contents\n\nEpi curve and maps\n\nUsed incidence and incidence2 for incidence calculation and visualisation\nraster to extract spatial information from library of shapefiles\n\nReproduction number\n\nAPEestim\nbayEStim\nearlyR\nepicontacts\nepidemia\nepiFilter\nEpiNow2\nEpiEstim\nR0\noutbreaker2\nUsed this comparison table to choose the most appropriate package.\n\nSuperspreading, by using these resources:\n\nfitdistrplus\nepicontacts\n\nEpidemic projections\n\nincidence R estimation using a loglinear model\nprojections using Rt estimates, SI distributions and overdispersion estimates\n\nTransmission chains and strain characterisation\n\nIQtree and nextclade to build a maximum likelihood tree and mannually inspect it\nAdvanced modelling through phylodynamic methods, using tools like BEAST\n\n\n\n\n\n\n\n\n\nData analysis step\nChallenges\n\n\n\n\nData cleaning\nNot knowing what packages are available for this purpose\n\n\nDelay distributions\nDealing with right truncation\nAccounting for multiple infectors\n\n\nPopulation demographics\nLacking tools that provide information about population by age, gender, etc.\n\n\nRisk factors of infection\nDistinguishing between risk factors vs detecting differences in reporting frequencies among groups\n\n\nSeverity of disease\nKnowing the prevalence of disease (denominator)\nRight truncated data\nVarying severity of different strains\n\n\nContact matching\nMissing data\nMisspellings\n\n\nEpicurve and maps\nNA dates entries not included\nReporting levels varying over time\n\n\nOffspring distribution\nRight truncation\nTime varying reporting efforts\nAssumption of a single homogeneous epidemic\nImportation of cases\n\n\nForecasting\nUnderlying assumption of a given R distribution, e.g., single trend, homogeneous mixing, no saturation"
  },
  {
    "objectID": "posts/100days-workshop/index.html#scenario-2-outbreak-of-an-unidentified-disease-in-rural-colombia",
    "href": "posts/100days-workshop/index.html#scenario-2-outbreak-of-an-unidentified-disease-in-rural-colombia",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "Scenario 2: Outbreak of an unidentified disease in rural Colombia",
    "text": "Scenario 2: Outbreak of an unidentified disease in rural Colombia\n\n\n\nScenario 2 details\n\n\n\nAnalytic pipeline for scenario 2 (analysed by group 3)\n\nData cleaning: manually, using R (no packages specified), to\n\nFix data entry issues in columns onset_date and gender\nCheck for missing data\nCheck sequence of dates: symptom onset → hospitalisation → death\n\nData anonymisation to share with partners\n\nfastlink for probabilistic matching between cases ↔︎ contacts, based on names, dates, and ages\n\nCase demographics\n\napyramid to stratify data by age, gender, and health status\n\nReproductive number calculation, by using two approaches:\n\nManually, by calculating the number of cases generated by each source case, data management through dplyr and data.table\nUsing serial interval of disease, through EpiEstim or EpiNow2\n\nSeverity of disease\n\nManual calculation of CFR and hospitalisation ratio\n\nProjection of hospital bed requirements\n\nEpiNow2 to calculate average hospitalisation duration and forecasting\n\nZoonotic transmission of disease\n\nManual inspection of cases’ occupation\nUse of IQtree and ggtree to plot phylogenetic data\n\nSuperspreading\n\nepicontacts\n\nCalculation of attack rate\n\nUnable to calculate, given the lack of seroprevalence data\n\n\n\n\n\n\n\n\n\nData analysis step\nChallenges\n\n\n\n\nData anonymisation\nDealing with typos and missing data when generating random unique identifiers\n\n\nReproduction number\nRight truncation\nUnderestimation of cases due to reporting delays\n\n\nProjection of hospital bed requirements\nIncomplete data (missing discharge date)\nUndocumented functionality in R packages used\n\n\nZoonotic transmission\nPoor documentation\nUnavailability of packages in R\nDifferentiation between zoonotic transmission and risk factors- need for population data\n\n\nAttack rate\nNot enough information provided"
  },
  {
    "objectID": "posts/100days-workshop/index.html#scenario-3-reston-ebolavirus-in-the-philippines",
    "href": "posts/100days-workshop/index.html#scenario-3-reston-ebolavirus-in-the-philippines",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "Scenario 3: Reston Ebolavirus in the Philippines",
    "text": "Scenario 3: Reston Ebolavirus in the Philippines\n\n\n\nScenario 3 details\n\n\n\nAnalytic pipeline for scenario 3 (analysed by group 4)\n\nData cleaning\n\nImporting data with rio, readxl, readr, or openxlsx\nRename variables with janitor\nInitial data checks with pointblank, assertr, compareDF, or skimr\nVertical data checks with matchmaker, lubridate, or parsedate\nHorizontal data checks with hmatch, assertr, or queryR\nDetect duplicates with janitor and tidyverse\nChecking for consistency with dplyr, or powerjoin\nTranslation with matchmaker\n\nDelay distributions\n\nfitdistrplus to fit parameteric distributions to epidemic data\n\nCase demographics\n\napyramid to stratify data by age, gender, and health status\nggplot2 to visualise data\n\nOutbreak description\n\nsitrep to generate reports\n\nVisualisation of geographic data\n\nsf for static maps\nleaflet for interactive maps\n\nGeneration of tables\n\ngtsummary for static tables\njanitor for interactive tables\n\nSeverity of disease\n\nEpiNow2 and survival to calculate CFR\n\nAttack rate\n\ngadm function to get population data\nepitabulate to describe data\nsf and ggplot2 to plot data\n\nForecasting\n\nEpiEstim\nEpiNow2\nbpmodels\n\nSpillover events\n\nBy cross-referencing contact data with occupations\n\nEffectiveness of contact tracing\n\nBy calculating the proportion of case follow-ups and comparing the delay of disease exposure to the follow-up delay\n\nTransmission trees\n\nepicontacts\nggplot2\n\n\n\n\n\nData analysis step\nChallenges\n\n\n\n\nDetection of outliers\nNo known tools to use\n\n\nSeverity of disease\nCensoring\n\n\nSpillover events\nMissing data"
  },
  {
    "objectID": "posts/100days-workshop/index.html#scenario-4-emerging-avian-influenza-in-cambodia",
    "href": "posts/100days-workshop/index.html#scenario-4-emerging-avian-influenza-in-cambodia",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "Scenario 4: Emerging avian influenza in Cambodia",
    "text": "Scenario 4: Emerging avian influenza in Cambodia\n\n\n\nScenario 4 details\n\n\n\nAnalytic pipeline for scenario 4 (analysed by group 5)\n\nData cleaning\n\ntidyverse\nreadxl to import data\ndplyr to remove names\nlubridate to standardise date formats\nManually scanning through excel to check for errors\n\nReproduction number\n\nEpiEstim\n\nSeverity of disease\n\nManually using R to detect missing cases\nepiR to check for data censoring\n\n\n\n\n\n\n\n\n\nData analysis step\nChallenges\n\n\n\n\nData cleaning\nNo available R packages specific for epidemic data\n\n\nReproduction number\nDifficulty finding parameter estimations in the literature\n\n\nSerial interval\nLack of a tool to check for parameter estimates\n\n\nSeverity\nMissing cases\nNeed for an R package for systematic censoring analysis"
  },
  {
    "objectID": "posts/100days-workshop/index.html#scenario-5-outbreak-of-respiratory-disease-in-canada",
    "href": "posts/100days-workshop/index.html#scenario-5-outbreak-of-respiratory-disease-in-canada",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "Scenario 5: Outbreak of respiratory disease in Canada",
    "text": "Scenario 5: Outbreak of respiratory disease in Canada\n\n\n\nScenario 5 details\n\n\n\nAnalytic pipeline for scenario 5 (analysed by group 1)\n\nDefine project structure\n\nDefining the script’s structure with cookiecutter, reportfactory, and orderly\nEnsuring reproducibility of the analysis with iRODS and Git\nWorking in a group with GitHub\n\nData cleaning\n\nImporting data with readr or rio\nChecking for errors with linelist, janitor, parsedate, matchmaker, or lubridate\njanitor to eliminate duplicates\nnaniar to check for missing data\nepitrix to anonymise data\n\nDelay distributions\n\nepitrix\nfitdistrplus to fit parameteric distributions to scenario data\n\nCase demographics\n\napyramid to stratify data by age, gender, and health status\n\nNowcasting\n\nincidence2 to visualise incidence from linelist data\nepiparameter to extract infectious disease parameter data\nEpiEstim or EpiNow2 for Rt calculation\n\nSeverity of disease\n\nCalculation of hospitalisation and mortality rates- no R package specified\n\nZoonotic transmission\n\nforecast\n\nGeneration of reports\n\nincidence for static reports\nQuarto and R markdown for dashboards\n\n\n\n\n\n\n\n\n\nData analysis step\nChallenges\n\n\n\n\nProject structure\nWorking simultaneously on the same script and managing parallel tasks\nAnticipating future incoming data in early pipeline design\n\n\nData cleaning\nLarge amount of code lines used on (reasonably) predictable cleaning (e.g. data sense checks)\nOmitting too many data entries when simply removing NA rows\nNon standardised data formats\nImplementing rapid quality check reports before analysis\n\n\nDelay distributions\nIdentifying the best method to calculate, or compare functionality of tools\nNeed to fit multiple parametric distributions and return best, and store as usable objects\n\n\nSeverity of disease\nCensoring and truncation\nUnderestimation of mild cases\nNeed database of age/gender pyramids for comparisons\n\n\nForecasts\nNeed option for fitting with range of plausible pathogen serial intervals and comparing results\nChanging reporting delays over time\nMatching inputs/outputs between packages\n\n\nZoonotic transmisison\nNeed for specific packages with clear documentation\nHow to compare simple trend-based forecasts"
  },
  {
    "objectID": "posts/100days-workshop/index.html#what-next",
    "href": "posts/100days-workshop/index.html#what-next",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "What next?",
    "text": "What next?\nScenarios developed by the 100 days workshop participants illustrate that there are many commonalities across proposed analytics pipelines, which could support interoperability across different epidemiological questions. However, there are also several remaining gaps and challenges, which creates an opportunity to build on existing work to tackle common outbreak scenarios, using the issues here as a starting point. This will also require consideration of wider interactions with existing software ecosystems and users of outbreak analytics insights. We are therefore planning to follow up this vignette with a more detailed perspective article discussing potential for broader progress in developing a ‘first 100 lines of code’."
  },
  {
    "objectID": "posts/100days-workshop/index.html#list-of-contributors",
    "href": "posts/100days-workshop/index.html#list-of-contributors",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "List of contributors",
    "text": "List of contributors\n\nGroup 1: Rich Fitzjohn, Mauricio Santos Vega, Andrea Torneri, Abdoelnaser Degoot, Rolina van Gaalen, Zulma Cucunuba, Joseph Tsui, Claudine Lim, Adam Kucharski.\nGroup 2: Juan Daniel Umaña, Joel Hellewell, Anne Cori, Fanck Kalala, Amrish Baidjoe, Sara Hollis, Chaoran Chen, Pratik Gupte, Andree Valle.\nGroup 3: Mutono Nyamai, Finlay Campbell, Arminder Deol, Simone Carter, Anita Shah, Neale Batra, Issa Karambal, Danil Mihailov, Sebastian Funk.\nGroup 4: Anton Camacho, Louise Dyson, Jeremy Bingham, Simon Cauchemez, Alex Spina, Esther Van Kleef, Anna Carnegie, James Azam.\nGroup 5: Olivia Keiser, Geraldine Gomez, John Lees, Don Klinkenberg, Matthew Biggerstaff, David Santiago Quevedo, Joshua Lambert, Carmen Tamayo."
  },
  {
    "objectID": "posts/s3-generic/index.html",
    "href": "posts/s3-generic/index.html",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "",
    "text": "To build a tight and well-integrated data pipeline, it may be desirable to rely on object orientation (OO) to automatically pass valuable information from one step to the other. OO and data classes can also act as a compatibility layer standardising outputs from various tools under a common structure.\nBut many packages and software start as standalone projects, and don’t always stem from a careful consideration of the larger ecosystem. In this situation, developers often see little benefit of using an OO system in their project initially.\nBut as the project matures, and as the position of the tool in the wider ecosystem becomes clearer, they may want to start using OO to benefit from the better integration it may provide with other tools upstream and downstream in the data pipeline. However, by then, their tool likely has an established community of users, and it is important to tread carefully with breaking changes.\nIn this blog post, we show that it’s possible to start using an S3 OO system almost invisibly in your R package, with minimal disruption to your users. We detail some minor changes that will nonetheless occur, and which pitfalls you should be looking out for. Finally, we take a step back and reflect how you should ensure you are a good open-source citizen in this endeavour."
  },
  {
    "objectID": "posts/s3-generic/index.html#all-methods-must-have-the-same-arguments-as-the-generic",
    "href": "posts/s3-generic/index.html#all-methods-must-have-the-same-arguments-as-the-generic",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "All methods must have the same arguments as the generic",
    "text": "All methods must have the same arguments as the generic\nYou can see that the method for pointset class, centroid.pointset() has a weights argument, even though it is not used because weights are already contained in the coords object. This seems clunky and potentially confusing for users. But this is mandatory because all methods must have the same arguments as the generic.\nAnother option here could have been to remove weights from the generic, and add ... instead, thus allowing to pass weights as an extra argument only in selected methods. This is more idiomatic in R, and in line with the recommendation from the official ‘Writing R Extensions’ document (“always keep generics simple”):\n\n#' @export\ncentroid &lt;- function(coords, ...) { \n  UseMethod(\"centroid\") \n}\n\n#' @rdname centroid\n#' \n#' @export\ncentroid.default &lt;- function(coords, weights, ...) {\n\n  coords_mat &lt;- do.call(rbind, coords)\n  \n  return(apply(coords_mat, 2, weighted.mean, w = weights))\n  \n}\n\nBut this extra ... argument, which is documented as “ignored”, may be confusing as well."
  },
  {
    "objectID": "posts/s3-generic/index.html#more-complex-documentation-presentation",
    "href": "posts/s3-generic/index.html#more-complex-documentation-presentation",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "More complex documentation presentation",
    "text": "More complex documentation presentation\nOn the topic of arguments, another pitfall related to the conversion to an S3 generic is the change in the documentation. Below is a collage of before / after the change. This is quite minor and some users may not even notice it but I remember it was very confusing to me when I started using R and I didn’t really know what S3 or OO was: “what do you mean, ‘Default S3 method’, which case applies to me?”\n\n\n\n\n\n\nScreenshot of the centroid() documentation before conversion to an S3 generic\n\n\n\n\n\n\n\nScreenshot of the centroid() documentation after conversion to an S3 generic\n\n\n\n\n\nThe answer is that “Default S3 method” lists the arguments for centroid.default(), i.e., the method which is used if no other method is defined for your class. Arguments for all methods are usually documented together but you should only focus on those present in the call after the comment stating “S3 method for class ‘XXX’” for the class you’re working with."
  },
  {
    "objectID": "posts/s3-generic/index.html#more-complicated-error-traceback",
    "href": "posts/s3-generic/index.html#more-complicated-error-traceback",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "More complicated error traceback",
    "text": "More complicated error traceback\nAnother situation where converting to an S3 adds an extra layer of complexity is where you are trying to follow the error traceback:\n\ncentroid(3)\n\n\nIn this example, we see one extra line that did not exist when centroid() was a regular function, rather than a generic:\n\ncentroid.default(3) at centroid.R#19\n\nThis line corresponds to the dispatch operation.\nHowever, this slight difference in behaviour is likely not a big issue as we mostly expect experienced users to interact with the traceback. These users are likely to be familiar with S3 dispatch and understand the traceback in any case."
  },
  {
    "objectID": "posts/s3-generic/index.html#extra-source-of-bugs-during-dispatch",
    "href": "posts/s3-generic/index.html#extra-source-of-bugs-during-dispatch",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "Extra source of bugs during dispatch",
    "text": "Extra source of bugs during dispatch\nOn a related note, the extra step introduced by this conversion to generic is another potential source of bugs. This doesn’t really impact your users directly but it does mean that as a developer, you will maintaining slightly more complex code and you will need to be more careful when making any changes. However, as always, a robust testing suite should help you catch any error before it makes it to production."
  },
  {
    "objectID": "posts/s3-generic/index.html#where-should-the-generic-live",
    "href": "posts/s3-generic/index.html#where-should-the-generic-live",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "Where should the generic live?",
    "text": "Where should the generic live?\nThe generic should always live in the package implementing the actual computation in the function in the first place. For example, if you defined the original centroid() function in a package called geometryops, the S3 generic should also be defined in that package, not in the package defining the pointset class.\nIt is possible in theory to overwrite a function defined by another package with a generic (“overloading”). For example, we could overload base R table() function with:\n\ntable &lt;- function(...) { \n  UseMethod(...)\n}\n\ntable.default &lt;- function(\n  ...,\n  exclude = if (useNA == \"no\") c(NA, NaN),\n  useNA = c(\"no\", \"ifany\", \"always\"),\n  dnn = list.names(...), deparse.level = 1\n) {\n\n base::table(\n  ...,\n  exclude = exclude,\n  useNA = useNA,\n  dnn = dnn\n )\n\n}\n\nBut this is generally considered bad practice, and possibly rude 2. As a rule of thumb, you should usually avoid:\n\nname collisions with functions from other packages (especially base or recommended package);\nlight wrappers around a function from another package as this may be seen as an attempt to steal citations and credit."
  },
  {
    "objectID": "posts/s3-generic/index.html#where-should-the-methods-live",
    "href": "posts/s3-generic/index.html#where-should-the-methods-live",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "Where should the methods live?",
    "text": "Where should the methods live?\nFor methods, there is more flexibility than for generics. They could either in the package defining the class, or in the package defining the generic. Let’s present the practical setup in both cases, as well as each strategy pros & cons.\n\nMethod in the class package\nThis is the strategy used when you defined a new class and provide it with a print(), a summary(), or a plot() method. The generics for these functions are defined in R base.\n\n#' @export\nplot.myclass &lt;- function(x, y, ...) {\n  \n  # code for a beautiful plot for your custom class\n  \n}\n\nIf you opt for this strategy, you will need to depend on the package providing the method, as Imports. For example, a package defining a fit.myclass() method for the fit() generic defined in the generics package would have the following DESCRIPTION and NAMESPACE.\n\n\nDESCRIPTION\n\nImports:\n  generics\n\n\n\nfit.myclass.R\n\n#' @export\n#' @importFrom generics fit\nfit.myclass &lt;- function(x, ...) {\n  # your code here\n}\n\n\n\nNAMESPACE\n\n# Generated by roxygen2: do not edit by hand\n\nS3method(fit,myclass)\nimportFrom(generics,fit)\n\n\n\n\n\n\n\nImporting the generic\n\n\n\nIt’s worth insisting that you need to import the generic in your NAMESPACE for the method to be recognized and exported correctly by roxygen2. In this specific situation, simply explicitly prefixing the generic call (generic::fit()) is not enough.\n\n\nBut this can lead to a rapid increase in the number of dependencies if you provide methods for generics from various packages. Since R 3.6, you can also put generics in Suggests and use delayed assignment:\n\n\nDESCRIPTION\n\nSuggests:\n  generics\n\n\n\nfit.myclass.R\n\n#' @exportS3Method generics::fit\nfit.myclass &lt;- function(x, ...) {\n  # your code here\n}\n\n\n\nNAMESPACE\n\n# Generated by roxygen2: do not edit by hand\n\nS3method(generics::fit,myclass)\n\n\n\nMethod in the generic package\nAlternatively, you can define the method in the package defining the generic. This is the approach taken in the report package from example, which defines the report() generic and methods for various model outputs produced by different package.\nIn theory, no Imports or Suggests is required here:\n\n#' @export\nmygeneric &lt;- function(x, ...) { \n  UseMethod(x)\n}\n\n#' @export\nmygeneric.externalclass &lt;- function(x, ...) {\n  # your code here\n}\n\nHowever, if you end up providing many methods for a specific class, you could put the package defining it in the uncommon Enhances field. Enhances is defined in ‘Writing R Extensions’ as:\n\nThe ‘Enhances’ field lists packages “enhanced” by the package at hand, e.g., by providing methods for classes from these packages.\n\nIt may be a good idea to explicitly signal the strong relationship between both packages so that the package defining the method is checked as a reverse dependency, and informed of potential breaking changes as discussed below. You may see an example of this in the slam package, which provides his methods for both base matrices and sparse matrices, as defined in the Matrix and the spam packages.\n\n\nCoordination between maintainers\nNo matter the strategy you end up choosing, we strongly recommend you keep an open communication channel between the class package and the generic package developer (provided they are not the same person) as breaking changes will impact both parties."
  },
  {
    "objectID": "posts/s3-generic/index.html#footnotes",
    "href": "posts/s3-generic/index.html#footnotes",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that we focus here on the S3 framework but R has other object orientation frameworks, as discussed in the relevant section of the ‘Advanced R’ book by Hadley Wickham↩︎\nEvery rule has its exceptions though such as the generics package, built by prominent members of the R developer community, which overloads base R functions such as as.factor() or as.difftime().↩︎"
  },
  {
    "objectID": "posts/statistical-correctness/index.html",
    "href": "posts/statistical-correctness/index.html",
    "title": "Ensuring & Showcasing the Statistical Correctness of your R Package",
    "section": "",
    "text": "We’re evolving in an increasingly data-driven world. And since critical decisions are taken based on results produced by data scientists and data analysts, they need to be be able to trust the tools they use. It is now increasingly common to add continuous integration to software packages and libraries, to ensure the code is not crashing, and that future updates don’t change your code output (snapshot tests). But one type of test still remains uncommon: tests for statistical correctness. That is, tests that ensure the algorithm implemented in your package actually produce the correct results.\nIt is likely that most statistical package authors run some tests on their own during development but there doesn’t seem to be guidelines on how to test statistical correctness in a solid and standard way 1.\nIn this blog post, we explore various methods to ensure the statistical correctness of your software. We argue that these tests should be part of your continuous integration system, to ensure your tools remains valid throughout its life, and to let users verify how you validate your package. Finally, we show how these principles are implemented in the Epiverse TRACE tools.\nThe approaches presented here are non-exclusive and should ideally all be added to your tests. However, they are presented in order of stringency and priority to implement. We also take a example of a function computing the centroid of a list of points to demonstrate how you would integrate the recommendations from this post with the {testthat} R package, often used from unit testing:\n#' Compute the centroid of a set of points\n#'\n#' @param coords Coordinates of the points as a list of vectors. Each element of the \n#'   list is a point.\n#'\n#' @returns A vector of coordinates of the same length of each element of \n#'   `coords`\n#'   \n#' @examples\n#' centroid(list(c(0, 1, 5, 3), c(8, 6, 4, 3), c(10, 2, 3, 7)))\n#' \ncentroid &lt;- function(coords) {\n\n  # ...\n  # Skip all the necessary input checking for the purpose of this demo\n  # ...\n\n  coords_mat &lt;- do.call(rbind, coords)\n  \n  return(colMeans(coords_mat))\n  \n}"
  },
  {
    "objectID": "posts/statistical-correctness/index.html#compare-your-results-to-the-reference-implementation",
    "href": "posts/statistical-correctness/index.html#compare-your-results-to-the-reference-implementation",
    "title": "Ensuring & Showcasing the Statistical Correctness of your R Package",
    "section": "Compare your results to the reference implementation",
    "text": "Compare your results to the reference implementation\nThe most straightforward and most solid way to ensure your implementation is valid is to compare your results to the results of the reference implementation. The reference implementation can be a package in another language, an example with toy data in the scientific article introducing the method, etc.\nFor example, the {gemma2} R package, which re-implements the methods from the GEMMA tool written in C++, verifies that values produced by both tools match:\ntest_that(\"Results of gemma2 equal those of GEMMA v 0.97\", {\n  expect_equal(Sigma_ee, diag(c(18.559, 12.3672)), tolerance = 0.0001)\n  expect_equal(Sigma_uu, diag(c(82.2973, 41.9238)), tolerance = 0.0001)\n})\n\n\n\n\n\n\nExample with centroid()\n\n\n\n\nlibrary(testthat)\n\ntest_that(\"centroid() in 1D produces the same results as mean()\", {\n\n  x &lt;- list(1, 5, 3, 10, 5)\n\n  expect_identical(centroid(x), mean(unlist(x)))\n  \n})\n\nTest passed 😸\n\n\n\n\nNote that even if a reference implementation doesn’t exist, it is still good practice to compare your implementation to competing ones. Discrepancies might reveal a bug in your implementation or theirs but in any case, finding it out is beneficial to the community.\nHowever, this approach cannot be used in all cases. Indeed, there may not be a reference implementation in your case. Or it might be difficult to replicate identical computations in the case of algorithm with stochasticity 2."
  },
  {
    "objectID": "posts/statistical-correctness/index.html#compare-to-a-theoretical-upper-or-lower-bound",
    "href": "posts/statistical-correctness/index.html#compare-to-a-theoretical-upper-or-lower-bound",
    "title": "Ensuring & Showcasing the Statistical Correctness of your R Package",
    "section": "Compare to a theoretical upper or lower bound",
    "text": "Compare to a theoretical upper or lower bound\nAn alternative strategy is to compare your result to theoretical upper or lower bound. This offers a weaker guarantee that your implementation and your results are correct but it can still allow you to detect important mistakes.\n\n\n\n\n\n\nExample with centroid()\n\n\n\n\ntest_that(\"centroid() is inside the hypercube containing the data points\", {\n  \n  x &lt;- list(c(0, 1, 5, 3), c(8, 6, 4, 3), c(10, 2, 3, 7))\n\n  expect_true(all(centroid(x) &lt;= Reduce(pmax, x)))\n  expect_true(all(centroid(x) &gt;= Reduce(pmin, x)))\n  \n})\n\nTest passed 😸\n\n\n\n\nYou can see a real-life example of this kind of test in the {finalsize} R package. {finalsize} computes the final proportion of infected in a heterogeneous population according to an SIR model. Theory predicts that the number of infections is maximal in a well-mixed population:\n# Calculates the upper limit of final size given the r0\n# The upper limit is given by a well mixed population\nupper_limit &lt;- function(r0) {\n  f &lt;- function(par) {\n    abs(1 - exp(-r0 * par[1]) - par[1])\n  }\n  opt &lt;- optim(\n    par = 0.5, fn = f,\n    lower = 0, upper = 1,\n    method = \"Brent\"\n  )\n  opt\n}"
  },
  {
    "objectID": "posts/statistical-correctness/index.html#verify-that-output-is-changing-as-expected-when-a-single-parameter-varies",
    "href": "posts/statistical-correctness/index.html#verify-that-output-is-changing-as-expected-when-a-single-parameter-varies",
    "title": "Ensuring & Showcasing the Statistical Correctness of your R Package",
    "section": "Verify that output is changing as expected when a single parameter varies",
    "text": "Verify that output is changing as expected when a single parameter varies\nAn even looser way to test statistical correctness would be to control that output varies as expected when you update some parameters. This could be for example, checking that the values you return increase when you increase or decrease one of your input parameters.\n\n\n\n\n\n\nExample with centroid()\n\n\n\n\ntest_that(\"centroid() increases when coordinates from one point increase\", {\n  \n  x &lt;- list(c(0, 1, 5, 3), c(8, 6, 4, 3), c(10, 2, 3, 7))\n  \n  y &lt;- x\n  y[[1]] &lt;- y[[1]] + 1 \n\n  expect_true(all(centroid(x) &lt; centroid(y)))\n  \n})\n\nTest passed 🌈\n\n\n\n\nAn example of this test in an actual R package can again be found in the finalsize package:\nr0_low &lt;- 1.3\nr0_high &lt;- 3.3\n\nepi_outcome_low &lt;- final_size(\n  r0 = r0_low,\n  &lt;...&gt;\n)\nepi_outcome_high &lt;- final_size(\n  r0 = r0_high,\n  &lt;...&gt;\n)\n\ntest_that(\"Higher values of R0 result in a higher number of infectious in all groups\", {\n  expect_true(\n    all(epi_outcome_high$p_infected &gt; epi_outcome_low$p_infected)\n  )\n})"
  },
  {
    "objectID": "posts/statistical-correctness/index.html#conclusion-automated-validation-vs-peer-review",
    "href": "posts/statistical-correctness/index.html#conclusion-automated-validation-vs-peer-review",
    "title": "Ensuring & Showcasing the Statistical Correctness of your R Package",
    "section": "Conclusion: automated validation vs peer-review",
    "text": "Conclusion: automated validation vs peer-review\nIn this post, we’ve presented different methods to automatically verify the statistical correctness of your statistical software. We would like to highlight one more time that it’s important to run these tests are part of your regular integration system, instead of running them just once at the start of the development. This will prevent the addition of possible errors in the code and show users what specific checks you are doing. By doing so, you are transparently committing to the highest quality.\nMultiple voices in the community are pushing more towards peer-review as a proxy for quality and validity:\n\nWe would like to highlight that automated validation and peer review are not mutually exclusive and answer slightly different purposes.\nOn the one hand, automated validation fails to catch more obscure bugs and edge cases. For example, a bug that would be difficult to detect via automated approach is the use of bad Random Number Generators when running in parallel.\nBut on the other hand, peer-review is less scalable, and journals usually have some editorial policy that might not make your package a good fit. Additionally, peer-review usually happens at one point in time while automated validation can, and should, be part of the continuous integration system.\nIdeally, peer-review and automated validation should work hand-in-hand, with review informing the addition of new automated validation tests."
  },
  {
    "objectID": "posts/statistical-correctness/index.html#footnotes",
    "href": "posts/statistical-correctness/index.html#footnotes",
    "title": "Ensuring & Showcasing the Statistical Correctness of your R Package",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut see the “testing statistical software” post from Alex Hayes where he presents his process to determine if he deems a statistical package trustworthy or not, and rOpenSci Statistical Software Peer Review book.↩︎\nSetting the random seed is not enough to compare implementations across programming languages because different languages use different kind of Random Number Generators.↩︎"
  },
  {
    "objectID": "learn.html",
    "href": "learn.html",
    "title": "Learn",
    "section": "",
    "text": "Introductory training on Git and GitHub basics using Rstudio\n\n\n\n\n\n\n\ndata science\n\n\ngit\n\n\ngithub\n\n\nrstudio\n\n\nnext trainings\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2022\n\n\nAndree Valle-Campos\n\n\n\n\n\n\nNo matching items"
  }
]