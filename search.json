[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "This blog is also available on R-bloggers.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nWhat Should the First 100 Lines of Code Written During an Epidemic Look Like?\n\n\n\n\n\n\n\noutbreak analytics\n\n\n100 days workshop\n\n\nparticipatory research\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2023\n\n\nCarmen Tamayo\n\n\n\n\n\n\n  \n\n\n\n\nConvert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations\n\n\n\n\n\n\n\nR\n\n\nR package\n\n\nobject orientation\n\n\nS3\n\n\ninteroperability\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nHugo Gruson\n\n\n\n\n\n\n  \n\n\n\n\nImproving the C++ Code Quality of an Rcpp Package\n\n\n\n\n\n\n\ncode quality\n\n\nR\n\n\nR package\n\n\nRcpp\n\n\n\n\n\n\n\n\n\n\n\nFeb 16, 2023\n\n\nPratik Gupte\n\n\n\n\n\n\n  \n\n\n\n\nEnsuring & Showcasing the Statistical Correctness of your R Package\n\n\n\n\n\n\n\ncode quality\n\n\nR\n\n\nR package\n\n\ntesting\n\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2023\n\n\nHugo Gruson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/harmonious-ecosystem/index.html",
    "href": "slides/harmonious-ecosystem/index.html",
    "title": "From disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project",
    "section": "",
    "text": "This presentation was given as part of deRSE23 - Conference for Research Software Engineering in Germany, in the Integration vs. Modularity session."
  },
  {
    "objectID": "slides/harmonious-ecosystem/index.html#abstract",
    "href": "slides/harmonious-ecosystem/index.html#abstract",
    "title": "From disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project",
    "section": "Abstract",
    "text": "Abstract\nThere is an increasing trend of packaging and sharing tools in the epidemiology research community. But these tools remain difficult to use and to integrate in a data analysis pipeline. There is a need for a more integrated approach, ensuring that the various tools work well with one another. For example, minimal data wrangling should be needed to transform the output of one tool before passing it to the next tool down the data analysis pipeline. Similarly, various alternatives for a single step of the pipeline should as much as possible use the same inputs and return the same outputs. In this talk, I will present the Epiverse-TRACE project, which collaborates with many R package developers in epidemiology to integrate their tools in a unified universe. Indeed, the unique and challenging feature of Epiverse is that it doesn’t intend to create a unified universe from scratch, but instead aims at updating existing external pieces of software to better work together. This talk will explain how we identify the key parts that should be updated, and how we make these updates with minimal disruption to the individual package developers and established community of users."
  },
  {
    "objectID": "slides/harmonious-ecosystem/index.html#slides",
    "href": "slides/harmonious-ecosystem/index.html#slides",
    "title": "From disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "slides/harmonious-ecosystem/index.html#questions-from-the-audience",
    "href": "slides/harmonious-ecosystem/index.html#questions-from-the-audience",
    "title": "From disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project",
    "section": "Questions from the audience",
    "text": "Questions from the audience\n\nHow do you incentivize developers to collaborate with you?\n\nAs mentioned in the presentation, we argue that collaboration results in a lower maintenance load for each developer by externalising and sharing the maintenance load of common elements.\nWe also think this is the logical continuation of releasing an open-source piece of software. If the goal is to provide a service to the community, then this service is surely greater if we collaborate to make the various pieces interoperable.\nAs an anecdotal piece of evidence, Nick Tierney, the maintainer of the conmat R package, has been very keen to participate in this project and was immediately convinced of the relevance and benefit for users and the ecosystem as a whole.\n\nHow do you ensure the long-term sustainability of this project?\n\nThis is an important question in the domain of open-source and research software but this is not the specific focus of this project and we don’t provide specific solutions, besides conforming to best practices.\nHowever, if anything, our projects should be more sustainable by construction since they result from a collaborative work and include multiple maintainers. This is also encoded in our projects by creating dedicated GitHub organisations, where all developers can participate on an equal footing, thereby also increasing the lottery factor."
  },
  {
    "objectID": "slides/harmonious-ecosystem/index.html#see-also",
    "href": "slides/harmonious-ecosystem/index.html#see-also",
    "title": "From disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project",
    "section": "See also",
    "text": "See also\n\nThe companion post on our blog"
  },
  {
    "objectID": "slides/showcase-winter2023/index.html",
    "href": "slides/showcase-winter2023/index.html",
    "title": "Epiverse-TRACE Winter 2023 showcase",
    "section": "",
    "text": "This showcase is part of a regular cycle of online meetings to present tools for epidemiology."
  },
  {
    "objectID": "slides/showcase-winter2023/index.html#finalsize-slides",
    "href": "slides/showcase-winter2023/index.html#finalsize-slides",
    "title": "Epiverse-TRACE Winter 2023 showcase",
    "section": "finalsize slides",
    "text": "finalsize slides"
  },
  {
    "objectID": "slides/showcase-winter2023/index.html#epiparameter-slides",
    "href": "slides/showcase-winter2023/index.html#epiparameter-slides",
    "title": "Epiverse-TRACE Winter 2023 showcase",
    "section": "epiparameter slides",
    "text": "epiparameter slides"
  },
  {
    "objectID": "slides/showcase-winter2023/index.html#episoap-slides",
    "href": "slides/showcase-winter2023/index.html#episoap-slides",
    "title": "Epiverse-TRACE Winter 2023 showcase",
    "section": "episoap slides",
    "text": "episoap slides"
  },
  {
    "objectID": "posts/100days-workshop/index.html",
    "href": "posts/100days-workshop/index.html",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "",
    "text": "This vignette summarises the findings from the 100 days and 100 lines of code workshop, hosted in December 2022 by Epiverse-TRACE.\nTo discuss how the first 100 lines of code during an epidemic should look like, we invited 40 experts, including academics, field epidemiologists, and software engineers, to take part in a 3-day workshop, where they discussed the current challenges, and potential solutions, in data analytic pipelines used to analyse epidemic data. In addition to highlighting existing technical solutions and their use cases, presentations on best practices in fostering collaboration across institutions and disciplines set the scene for the subsequent workshop scenario exercises."
  },
  {
    "objectID": "posts/100days-workshop/index.html#scenario-1-novel-respiratory-disease-in-the-gambia",
    "href": "posts/100days-workshop/index.html#scenario-1-novel-respiratory-disease-in-the-gambia",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "Scenario 1: Novel respiratory disease in The Gambia",
    "text": "Scenario 1: Novel respiratory disease in The Gambia\n\n\nAnalytic pipeline for scenario 1 (analysed by group 2)\n\nData cleaning\n\nlinelist to standardise date format\ncleanr from previous Hackathon\n\nDelay distributions\n\nfitdisrplus to fit parameteric distributions to scenario data\nepiparameter to extract delay distributions from respiratory pathogens\nEpiNow2 to fit reporting delays\nEpiEstim / coarseDataTools to estimate generation time/serial interval of disease\nepicontacts\nmixdiff to estimate delay distributions and correct erroneous dates at the same time (still under development)\n\nPopulation demographics\n\nWould like to have had access to an R package similar to ColOpenData\n\nRisk factors of infection\n\nUsed R4epis as a guide on how to create two-way tables and perform Chi-squared tests\n\nSeverity of disease\n\ndatadelay for CFR calculation\nImplementation of method developed by AC Ghani, 2005 to estimate CFR\n\nContact matching\n\ndiyar to match and link records\nfuzzyjoin to join contact and case data despite misspellings or missing cell contents\n\nEpi curve and maps\n\nUsed incidence and incidence2 for incidence calculation and visualisation\nraster to extract spatial information from library of shapefiles\n\nReproduction number\n\nAPEestim\nbayEStim\nearlyR\nepicontacts\nepidemia\nepiFilter\nEpiNow2\nEpiEstim\nR0\noutbreaker2\nUsed this comparison table to choose the most appropriate package.\n\nSuperspreading, by using these resources:\n\nfitdistrplus\nepicontacts\n\nEpidemic projections\n\nincidence R estimation using a loglinear model\nprojections using Rt estimates, SI distributions and overdispersion estimates\n\nTransmission chains and strain characterisation\n\nIQtree and nextclade to build a maximum likelihood tree and mannually inspect it\nAdvanced modelling through phylodynamic methods, using tools like BEAST\n\n\n\n\n\n\n\n\n\nData analysis step\nChallenges\n\n\n\n\nData cleaning\nNot knowing what packages are available for this purpose\n\n\nDelay distributions\nDealing with right truncation\nAccounting for multiple infectors\n\n\nPopulation demographics\nLacking tools that provide information about population by age, gender, etc.\n\n\nRisk factors of infection\nDistinguishing between risk factors vs detecting differences in reporting frequencies among groups\n\n\nSeverity of disease\nKnowing the prevalence of disease (denominator)\nRight truncated data\nVarying severity of different strains\n\n\nContact matching\nMissing data\nMisspellings\n\n\nEpicurve and maps\nNA dates entries not included\nReporting levels varying over time\n\n\nOffspring distribution\nRight truncation\nTime varying reporting efforts\nAssumption of a single homogeneous epidemic\nImportation of cases\n\n\nForecasting\nUnderlying assumption of a given R distribution, e.g., single trend, homogeneous mixing, no saturation"
  },
  {
    "objectID": "posts/100days-workshop/index.html#scenario-2-outbreak-of-an-unidentified-disease-in-rural-colombia",
    "href": "posts/100days-workshop/index.html#scenario-2-outbreak-of-an-unidentified-disease-in-rural-colombia",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "Scenario 2: Outbreak of an unidentified disease in rural Colombia",
    "text": "Scenario 2: Outbreak of an unidentified disease in rural Colombia\n\n\nAnalytic pipeline for scenario 2 (analysed by group 3)\n\nData cleaning: manually, using R (no packages specified), to\n\nFix data entry issues in columns onset_date and gender\nCheck for missing data\nCheck sequence of dates: symptom onset → hospitalisation → death\n\nData anonymisation to share with partners\n\nfastlink for probabilistic matching between cases ↔︎ contacts, based on names, dates, and ages\n\nCase demographics\n\napyramid to stratify data by age, gender, and health status\n\nReproductive number calculation, by using two approaches:\n\nManually, by calculating the number of cases generated by each source case, data management through dplyr and data.table\nUsing serial interval of disease, through EpiEstim or EpiNow2\n\nSeverity of disease\n\nManual calculation of CFR and hospitalisation ratio\n\nProjection of hospital bed requirements\n\nEpiNow2 to calculate average hospitalisation duration and forecasting\n\nZoonotic transmission of disease\n\nManual inspection of cases’ occupation\nUse of IQtree and ggtree to plot phylogenetic data\n\nSuperspreading\n\nepicontacts\n\nCalculation of attack rate\n\nUnable to calculate, given the lack of seroprevalence data\n\n\n\n\n\n\n\n\n\nData analysis step\nChallenges\n\n\n\n\nData anonymisation\nDealing with typos and missing data when generating random unique identifiers\n\n\nReproduction number\nRight truncation\nUnderestimation of cases due to reporting delays\n\n\nProjection of hospital bed requirements\nIncomplete data (missing discharge date)\nUndocumented functionality in R packages used\n\n\nZoonotic transmission\nPoor documentation\nUnavailability of packages in R\nDifferentiation between zoonotic transmission and risk factors- need for population data\n\n\nAttack rate\nNot enough information provided"
  },
  {
    "objectID": "posts/100days-workshop/index.html#scenario-3-reston-ebolavirus-in-the-philippines",
    "href": "posts/100days-workshop/index.html#scenario-3-reston-ebolavirus-in-the-philippines",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "Scenario 3: Reston Ebolavirus in the Philippines",
    "text": "Scenario 3: Reston Ebolavirus in the Philippines\n\n\nAnalytic pipeline for scenario 3 (analysed by group 4)\n\nData cleaning\n\nImporting data with rio, readxl, readr, or openxlsx\nRename variables with janitor\nInitial data checks with pointblank, assertr, compareDF, or skimr\nVertical data checks with matchmaker, lubridate, or parsedate\nHorizontal data checks with hmatch, assertr, or queryR\nDetect duplicates with janitor and tidyverse\nChecking for consistency with dplyr, or powerjoin\nTranslation with matchmaker\n\nDelay distributions\n\nfitdistrplus to fit parameteric distributions to epidemic data\n\nCase demographics\n\napyramid to stratify data by age, gender, and health status\nggplot2 to visualise data\n\nOutbreak description\n\nsitrep to generate reports\n\nVisualisation of geographic data\n\nsf for static maps\nleaflet for interactive maps\n\nGeneration of tables\n\ngtsummary for static tables\njanitor for interactive tables\n\nSeverity of disease\n\nEpiNow2 and survival to calculate CFR\n\nAttack rate\n\ngadm function to get population data\nepitabulate to describe data\nsf and ggplot2 to plot data\n\nForecasting\n\nEpiEstim\nEpiNow2\nbpmodels\n\nSpillover events\n\nBy cross-referencing contact data with occupations\n\nEffectiveness of contact tracing\n\nBy calculating the proportion of case follow-ups and comparing the delay of disease exposure to the follow-up delay\n\nTransmission trees\n\nepicontacts\nggplot2\n\n\n\n\n\nData analysis step\nChallenges\n\n\n\n\nDetection of outliers\nNo known tools to use\n\n\nSeverity of disease\nCensoring\n\n\nSpillover events\nMissing data"
  },
  {
    "objectID": "posts/100days-workshop/index.html#scenario-4-emerging-avian-influenza-in-cambodia",
    "href": "posts/100days-workshop/index.html#scenario-4-emerging-avian-influenza-in-cambodia",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "Scenario 4: Emerging avian influenza in Cambodia",
    "text": "Scenario 4: Emerging avian influenza in Cambodia\n\n\nAnalytic pipeline for scenario 4 (analysed by group 5)\n\nData cleaning\n\ntidyverse\nreadxl to import data\ndplyr to remove names\nlubridate to standardise date formats\nManually scanning through excel to check for errors\n\nReproduction number\n\nEpiEstim\n\nSeverity of disease\n\nManually using R to detect missing cases\nepiR to check for data censoring\n\n\n\n\n\n\n\n\n\nData analysis step\nChallenges\n\n\n\n\nData cleaning\nNo available R packages specific for epidemic data\n\n\nReproduction number\nDifficulty finding parameter estimations in the literature\n\n\nSerial interval\nLack of a tool to check for parameter estimates\n\n\nSeverity\nMissing cases\nNeed for an R package for systematic censoring analysis"
  },
  {
    "objectID": "posts/100days-workshop/index.html#scenario-5-outbreak-of-respiratory-disease-in-canada",
    "href": "posts/100days-workshop/index.html#scenario-5-outbreak-of-respiratory-disease-in-canada",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "Scenario 5: Outbreak of respiratory disease in Canada",
    "text": "Scenario 5: Outbreak of respiratory disease in Canada\n\n\nAnalytic pipeline for scenario 5 (analysed by group 1)\n\nDefine project structure\n\nDefining the script’s structure with cookiecutter, reportfactory, and orderly\nEnsuring reproducibility of the analysis with iRODS and Git\nWorking in a group with GitHub\n\nData cleaning\n\nImporting data with readr or rio\nChecking for errors with linelist, janitor, parsedate, matchmaker, or lubridate\njanitor to eliminate duplicates\nnaniar to check for missing data\nepitrix to anonymise data\n\nDelay distributions\n\nepitrix\nfitdistrplus to fit parameteric distributions to scenario data\n\nCase demographics\n\napyramid to stratify data by age, gender, and health status\n\nNowcasting\n\nincidence2 to visualise incidence from linelist data\nepiparameter to extract infectious disease parameter data\nEpiEstim or EpiNow2 for Rt calculation\n\nSeverity of disease\n\nCalculation of hospitalisation and mortality rates- no R package specified\n\nZoonotic transmission\n\nforecast\n\nGeneration of reports\n\nincidence for static reports\nQuarto and R markdown for dashboards\n\n\n\n\n\n\n\n\n\nData analysis step\nChallenges\n\n\n\n\nProject structure\nWorking simultaneously on the same script and managing parallel tasks\nAnticipating future incoming data in early pipeline design\n\n\nData cleaning\nLarge amount of code lines used on (reasonably) predictable cleaning (e.g. data sense checks)\nOmitting too many data entries when simply removing NA rows\nNon standardised data formats\nImplementing rapid quality check reports before analysis\n\n\nDelay distributions\nIdentifying the best method to calculate, or compare functionality of tools\nNeed to fit multiple parametric distributions and return best, and store as usable objects\n\n\nSeverity of disease\nCensoring and truncation\nUnderestimation of mild cases\nNeed database of age/gender pyramids for comparisons\n\n\nForecasts\nNeed option for fitting with range of plausible pathogen serial intervals and comparing results\nChanging reporting delays over time\nMatching inputs/outputs between packages\n\n\nZoonotic transmisison\nNeed for specific packages with clear documentation\nHow to compare simple trend-based forecasts"
  },
  {
    "objectID": "posts/100days-workshop/index.html#what-next",
    "href": "posts/100days-workshop/index.html#what-next",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "What next?",
    "text": "What next?\nScenarios developed by the 100 days workshop participants illustrate that there are many commonalities across proposed analytics pipelines, which could support interoperability across different epidemiological questions. However, there are also several remaining gaps and challenges, which creates an opportunity to build on existing work to tackle common outbreak scenarios, using the issues here as a starting point. This will also require consideration of wider interactions with existing software ecosystems and users of outbreak analytics insights. We are therefore planning to follow up this vignette with a more detailed perspective article discussing potential for broader progress in developing a ‘first 100 lines of code’."
  },
  {
    "objectID": "posts/100days-workshop/index.html#list-of-contributors",
    "href": "posts/100days-workshop/index.html#list-of-contributors",
    "title": "What Should the First 100 Lines of Code Written During an Epidemic Look Like?",
    "section": "List of contributors",
    "text": "List of contributors\n\nGroup 1: Rich Fitzjohn, Mauricio Santos Vega, Andrea Torneri, Abdoelnaser Degoot, Rolina van Gaalen, Zulma Cucunuba, Joseph Tsui, Claudine Lim, Adam Kucharski.\nGroup 2: Juan Daniel Umana, Joel Hellewell, Anne Cori, Fanck Kalala, Amrish Baidjoe, Sara Hollis, Chaoran Chen, Pratik Gupte, Andree Valle.\nGroup 3: Mutono Nyamai, Finlay Campbell, Arminder Deol, Simone Carter, Anita Shah, Neale Batra, Issa Karambal, Danil Mihailov, Sebastian Funk.\nGroup 4: Anton Camacho, Louise Dyson, Jeremy Bingham, Simon Cauchemez, Alex Spina, Esther Van Kleef, Anna Carnegie, James Azam.\nGroup 5: Olivia Keiser, Geraldine Gomez, John Lees, Don Klinkenberg, Matthew Biggerstaff, David Santiago Quevedo, Joshua Lambert, Carmen Tamayo."
  },
  {
    "objectID": "posts/statistical-correctness/index.html",
    "href": "posts/statistical-correctness/index.html",
    "title": "Ensuring & Showcasing the Statistical Correctness of your R Package",
    "section": "",
    "text": "We’re evolving in an increasingly data-driven world. And since critical decisions are taken based on results produced by data scientists and data analysts, they need to be be able to trust the tools they use. It is now increasingly common to add continuous integration to software packages and libraries, to ensure the code is not crashing, and that future updates don’t change your code output (snapshot tests). But one type of test still remains uncommon: tests for statistical correctness. That is, tests that ensure the algorithm implemented in your package actually produce the correct results.\nIt is likely that most statistical package authors run some tests on their own during development but there doesn’t seem to be guidelines on how to test statistical correctness in a solid and standard way 1.\nIn this blog post, we explore various methods to ensure the statistical correctness of your software. We argue that these tests should be part of your continuous integration system, to ensure your tools remains valid throughout its life, and to let users verify how you validate your package. Finally, we show how these principles are implemented in the Epiverse TRACE tools.\nThe approaches presented here are non-exclusive and should ideally all be added to your tests. However, they are presented in order of stringency and priority to implement. We also take a example of a function computing the centroid of a list of points to demonstrate how you would integrate the recommendations from this post with the {testthat} R package, often used from unit testing:\n#' Compute the centroid of a set of points\n#'\n#' @param coords Coordinates of the points as a list of vectors. Each element of the \n#'   list is a point.\n#'\n#' @returns A vector of coordinates of the same length of each element of \n#'   `coords`\n#'   \n#' @examples\n#' centroid(list(c(0, 1, 5, 3), c(8, 6, 4, 3), c(10, 2, 3, 7)))\n#' \ncentroid &lt;- function(coords) {\n\n  # ...\n  # Skip all the necessary input checking for the purpose of this demo\n  # ...\n\n  coords_mat &lt;- do.call(rbind, coords)\n  \n  return(colMeans(coords_mat))\n  \n}"
  },
  {
    "objectID": "posts/statistical-correctness/index.html#footnotes",
    "href": "posts/statistical-correctness/index.html#footnotes",
    "title": "Ensuring & Showcasing the Statistical Correctness of your R Package",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBut see the “testing statistical software” post from Alex Hayes where he presents his process to determine if he deems a statistical package trustworthy or not, and rOpenSci Statistical Software Peer Review book.↩︎\nSetting the random seed is not enough to compare implementations across programming languages because different languages use different kind of Random Number Generators.↩︎"
  },
  {
    "objectID": "posts/s3-generic/index.html",
    "href": "posts/s3-generic/index.html",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "",
    "text": "To build a tight and well-integrated data pipeline, it may be desirable to rely on object orientation (OO) to automatically pass valuable information from one step to the other. OO and data classes can also act as a compatibility layer standardising outputs from various tools under a common structure.\nBut many packages and software start as standalone projects, and don’t always stem from a careful consideration of the larger ecosystem. In this situation, developers often see little benefit of using an OO system in their project initially.\nBut as the project matures, and as the position of the tool in the wider ecosystem becomes clearer, they may want to start using OO to benefit from the better integration it may provide with other tools upstream and downstream in the data pipeline. However, by then, their tool likely has an established community of users, and it is important to tread carefully with breaking changes.\nIn this blog post, we show that it’s possible to start using an S3 OO system almost invisibly in your R package, with minimal disruption to your users. We detail some minor changes that will nonetheless occur, and which pitfalls you should be looking out for. Finally, we take a step back and reflect how you should ensure you are a good open-source citizen in this endeavour."
  },
  {
    "objectID": "posts/s3-generic/index.html#all-methods-must-have-the-same-arguments-as-the-generic",
    "href": "posts/s3-generic/index.html#all-methods-must-have-the-same-arguments-as-the-generic",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "All methods must have the same arguments as the generic",
    "text": "All methods must have the same arguments as the generic\nYou can see that the method for pointset class, centroid.pointset() has a weights argument, even though it is not used because weights are already contained in the coords object. This seems clunky and potentially confusing for users. But this is mandatory because all methods must have the same arguments as the generic.\nAnother option here could have been to remove weights from the generic, and add ... instead, thus allowing to pass weights as an extra argument only in selected methods. This is more idiomatic in R, and in line with the recommendation from the official ‘Writing R Extensions’ document (“always keep generics simple”):\n\n#' @export\ncentroid &lt;- function(coords, ...) { \n  UseMethod(\"centroid\") \n}\n\n#' @rdname centroid\n#' \n#' @export\ncentroid.default &lt;- function(coords, weights, ...) {\n\n  coords_mat &lt;- do.call(rbind, coords)\n  \n  return(apply(coords_mat, 2, weighted.mean, w = weights))\n  \n}\n\nBut this extra ... argument, which is documented as “ignored”, may be confusing as well."
  },
  {
    "objectID": "posts/s3-generic/index.html#more-complex-documentation-presentation",
    "href": "posts/s3-generic/index.html#more-complex-documentation-presentation",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "More complex documentation presentation",
    "text": "More complex documentation presentation\nOn the topic of arguments, another pitfall related to the conversion to an S3 generic is the change in the documentation. Below is a collage of before / after the change. This is quite minor and some users may not even notice it but I remember it was very confusing to me when I started using R and I didn’t really know what S3 or OO was: “what do you mean, ‘Default S3 method’, which case applies to me?”\n\n\n\n\n\n\nScreenshot of the centroid() documentation before conversion to an S3 generic\n\n\n\n\n\n\n\nScreenshot of the centroid() documentation after conversion to an S3 generic\n\n\n\n\n\nThe answer is that “Default S3 method” lists the arguments for centroid.default(), i.e., the method which is used if no other method is defined for your class. Arguments for all methods are usually documented together but you should only focus on those present in the call after the comment stating “S3 method for class ‘XXX’” for the class you’re working with."
  },
  {
    "objectID": "posts/s3-generic/index.html#more-complicated-error-traceback",
    "href": "posts/s3-generic/index.html#more-complicated-error-traceback",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "More complicated error traceback",
    "text": "More complicated error traceback\nAnother situation where converting to an S3 adds an extra layer of complexity is where you are trying to follow the error traceback:\n\ncentroid(3)\n\n\nIn this example, we see one extra line that did not exist when centroid() was a regular function, rather than a generic:\n\ncentroid.default(3) at centroid.R#19\n\nThis line corresponds to the dispatch operation.\nHowever, this slight difference in behaviour is likely not a big issue as we mostly expect experienced users to interact with the traceback. These users are likely to be familiar with S3 dispatch and understand the traceback in any case."
  },
  {
    "objectID": "posts/s3-generic/index.html#extra-source-of-bugs-during-dispatch",
    "href": "posts/s3-generic/index.html#extra-source-of-bugs-during-dispatch",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "Extra source of bugs during dispatch",
    "text": "Extra source of bugs during dispatch\nOn a related note, the extra step introduced by this conversion to generic is another potential source of bugs. This doesn’t really impact your users directly but it does mean that as a developer, you will maintaining slightly more complex code and you will need to be more careful when making any changes. However, as always, a robust testing suite should help you catch any error before it makes it to production."
  },
  {
    "objectID": "posts/s3-generic/index.html#where-should-the-generic-live",
    "href": "posts/s3-generic/index.html#where-should-the-generic-live",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "Where should the generic live?",
    "text": "Where should the generic live?\nThe generic should always live in the package implementing the actual computation in the function in the first place. For example, if you defined the original centroid() function in a package called geometryops, the S3 generic should also be defined in that package, not in the package defining the pointset class.\nIt is possible in theory to overwrite a function defined by another package with a generic (“overloading”). For example, we could overload base R table() function with:\n\ntable &lt;- function(...) { \n  UseMethod(...)\n}\n\ntable.default &lt;- function(\n  ...,\n  exclude = if (useNA == \"no\") c(NA, NaN),\n  useNA = c(\"no\", \"ifany\", \"always\"),\n  dnn = list.names(...), deparse.level = 1\n) {\n\n base::table(\n  ...,\n  exclude = exclude,\n  useNA = useNA,\n  dnn = dnn\n )\n\n}\n\nBut this is generally considered bad practice, and possibly rude 2. As a rule of thumb, you should usually avoid:\n\nname collisions with functions from other packages (especially base or recommended package);\nlight wrappers around a function from another package as this may be seen as an attempt to steal citations and credit."
  },
  {
    "objectID": "posts/s3-generic/index.html#where-should-the-methods-live",
    "href": "posts/s3-generic/index.html#where-should-the-methods-live",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "Where should the methods live?",
    "text": "Where should the methods live?\nFor methods, there is more flexibility than for generics. They could either in the package defining the class, or in the package defining the generic. Let’s present the practical setup in both cases, as well as each strategy pros & cons.\n\nMethod in the class package\nThis is the strategy used when you defined a new class and provide it with a print(), a summary(), or a plot() method. The generics for these functions are defined in R base.\n\n#' @export\nplot.myclass &lt;- function(x, y, ...) {\n  \n  # code for a beautiful plot for your custom class\n  \n}\n\nIf you opt for this strategy, you will need to depend on the package providing the method, as Imports. For example, a package defining a fit.myclass() method for the fit() generic defined in the generics package would have the following DESCRIPTION and NAMESPACE.\n\n\nDESCRIPTION\n\nImports:\n  generics\n\n\n\nfit.myclass.R\n\n#' @export\n#' @importFrom generics fit\nfit.myclass &lt;- function(x, ...) {\n  # your code here\n}\n\n\n\nNAMESPACE\n\n# Generated by roxygen2: do not edit by hand\n\nS3method(fit,myclass)\nimportFrom(generics,fit)\n\n\n\n\n\n\n\nImporting the generic\n\n\n\nIt’s worth insisting that you need to import the generic in your NAMESPACE for the method to be recognized and exported correctly by roxygen2. In this specific situation, simply explicitly prefixing the generic call (generic::fit()) is not enough.\n\n\nBut this can lead to a rapid increase in the number of dependencies if you provide methods for generics from various packages. Since R 3.6, you can also put generics in Suggests and use delayed assignment:\n\n\nDESCRIPTION\n\nSuggests:\n  generics\n\n\n\nfit.myclass.R\n\n#' @exportS3Method generics::fit\nfit.myclass &lt;- function(x, ...) {\n  # your code here\n}\n\n\n\nNAMESPACE\n\n# Generated by roxygen2: do not edit by hand\n\nS3method(generics::fit,myclass)\n\n\n\nMethod in the generic package\nAlternatively, you can define the method in the package defining the generic. This is the approach taken in the report package from example, which defines the report() generic and methods for various model outputs produced by different package.\nIn theory, no Imports or Suggests is required here:\n\n#' @export\nmygeneric &lt;- function(x, ...) { \n  UseMethod(x)\n}\n\n#' @export\nmygeneric.externalclass &lt;- function(x, ...) {\n  # your code here\n}\n\nHowever, if you end up providing many methods for a specific class, you could put the package defining it in the uncommon Enhances field. Enhances is defined in ‘Writing R Extensions’ as:\n\nThe ‘Enhances’ field lists packages “enhanced” by the package at hand, e.g., by providing methods for classes from these packages.\n\nIt may be a good idea to explicitly signal the strong relationship between both packages so that the package defining the method is checked as a reverse dependency, and informed of potential breaking changes as discussed below. You may see an example of this in the slam package, which provides his methods for both base matrices and sparse matrices, as defined in the Matrix and the spam packages.\n\n\nCoordination between maintainers\nNo matter the strategy you end up choosing, we strongly recommend you keep an open communication channel between the class package and the generic package developer (provided they are not the same person) as breaking changes will impact both parties."
  },
  {
    "objectID": "posts/s3-generic/index.html#footnotes",
    "href": "posts/s3-generic/index.html#footnotes",
    "title": "Convert Your R Function to an S3 Generic: Benefits, Pitfalls & Design Considerations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that we focus here on the S3 framework but R has other object orientation frameworks, as discussed in the relevant section of the ‘Advanced R’ book by Hadley Wickham↩︎\nEvery rule has its exceptions though such as the generics package, built by prominent members of the R developer community, which overloads base R functions such as as.factor() or as.difftime().↩︎"
  },
  {
    "objectID": "posts/lint-rcpp/index.html",
    "href": "posts/lint-rcpp/index.html",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "",
    "text": "The R package development ecosystem includes packages such as {lintr} and {styler} that can help to check code style, and to format R code.\nHowever, these packages cannot lint or style the C++ code of {Rcpp} packages. This could leave the C++ code of an Rcpp package less clean than the R code, increasing the technical debt already associated with using two languages.\nIn Epiverse-TRACE, we encounter this issue with {finalsize}, and we anticipate the same issue with further epidemic modelling packages that we seek to develop or adapt, such as {fluEvidenceSynthesis}.\nOur use-case is not unique, of course, and other projects could have their own solutions. One such, from which we have borrowed some ideas, is the Apache Arrow project, whose R package also uses a C++ backend (via {cpp11} rather than {Rcpp})."
  },
  {
    "objectID": "posts/lint-rcpp/index.html#use-case",
    "href": "posts/lint-rcpp/index.html#use-case",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "",
    "text": "The R package development ecosystem includes packages such as {lintr} and {styler} that can help to check code style, and to format R code.\nHowever, these packages cannot lint or style the C++ code of {Rcpp} packages. This could leave the C++ code of an Rcpp package less clean than the R code, increasing the technical debt already associated with using two languages.\nIn Epiverse-TRACE, we encounter this issue with {finalsize}, and we anticipate the same issue with further epidemic modelling packages that we seek to develop or adapt, such as {fluEvidenceSynthesis}.\nOur use-case is not unique, of course, and other projects could have their own solutions. One such, from which we have borrowed some ideas, is the Apache Arrow project, whose R package also uses a C++ backend (via {cpp11} rather than {Rcpp})."
  },
  {
    "objectID": "posts/lint-rcpp/index.html#choice-of-c-linters",
    "href": "posts/lint-rcpp/index.html#choice-of-c-linters",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Choice of C++ linters",
    "text": "Choice of C++ linters\nC++ linters such as clang-tidy stumble when dealing with C++ code in src/, as the clang toolchain attempts to compile it. This does not work for Rcpp packages, as the Rcpp.h header cannot be found — this linking is handled by {Rcpp}.\nFortunately, other C++ linters and code checking tools are available and can be used safely with Rcpp packages.\nWe have chosen to use cpplint and cppcheck for {finalsize}.\n\nCpplint\ncpplint is a tool that checks whether C/C++ files follow Google’s C++ style guide. cpplint is easy to install across platforms, and does not error when it cannot find Rcpp.h.\nImportantly, cpplint can be instructed to not lint the autogenerated RcppExports.cpp file, which follows a different style.\nTo lint all other .cpp files, we simply run cpplint from the terminal.\ncpplint --exclude=\"src/RcppExports.cpp\" src/*.cpp\n\n\nCppcheck\ncppcheck is a static code analysis tool, that aims to “have very few false positives”. This is especially useful for the non-standard organisation of Rcpp projects compared to C++ projects.\ncppcheck can also be run locally and instructed to ignore the autogenerated RcppExports.cpp file, while throwing up issues with style.\ncppcheck -i src/RcppExports.cpp --enable=style --error-exitcode=1 src\nHere, the --enable=style option lets cppcheck flag issues with style, acting as a second linter. This enables the performance and portability flags as well. (We have not found any difference when using --enable=warning instead.)\nEnabling all checks (--enable=all) would flag two specific issues for {Rcpp} packages: (1) the Rcpp*.h headers not being found (of the class missingIncludeSystem), and (2) the solver functions not being used by any other C++ function (unusedFunction).\nThese extra options should be avoided in {Rcpp} packages, as the linking is handled for us, and the functions are indeed used later — just not by other C++ functions.\nThe --error-exitcode=1 argument returns the integer 1 when an error is found, which is by convention the output for an error."
  },
  {
    "objectID": "posts/lint-rcpp/index.html#adding-c-linting-to-ci-workflows",
    "href": "posts/lint-rcpp/index.html#adding-c-linting-to-ci-workflows",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Adding C++ linting to CI workflows",
    "text": "Adding C++ linting to CI workflows\nBoth cpplint and cppcheck can be easily added to continuous integration workflows. In Epiverse-TRACE, we use Github Actions. The C++ lint workflow we have implemented looks like this:\non:\n  push:\n    paths: \"src/**\"\n  pull_request:\n    branches:\n      - \"*\"\n\nname: Cpp-lint-check\n\njobs:\n  cpplint:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v2\n      - run: pip install cpplint\n      - run: cpplint --quiet --exclude=\"src/RcppExports.cpp\" src/*.cpp\n\n  cppcheck:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - run: sudo apt-get install cppcheck\n      - run: cppcheck -i src/RcppExports.cpp --quiet --enable=warning --error-exitcode=1 .\nThe workflow is triggered when there are changes to files in src/, and on all pull requests."
  },
  {
    "objectID": "posts/lint-rcpp/index.html#formatting-c-code",
    "href": "posts/lint-rcpp/index.html#formatting-c-code",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Formatting C++ code",
    "text": "Formatting C++ code\nC++ code can be automatically formatted to avoid linter errors. An especially useful tool is clang-format. Our code is styled to follow the Google C++ style guide using:\n# replace .cpp with .h to format headers\nclang-format -i -style=google src/*.cpp\nHowever, this also formats the autogenerated RcppExports.cpp file. It can be extra work to repeatedly undo this change and keep the original formatting, but clang-format does not provide an easy inline way to ignore this file.\nInstead, clang-format can be passed all files except RcppExports.cpp to style using some simple shell commands. In smaller projects, it might be worth\nfind src -name \"*.cpp\" ! -name \"RcppExports.cpp\" -exec clang-format -style=google -i {} \\;"
  },
  {
    "objectID": "posts/lint-rcpp/index.html#turning-off-linting-and-formatting",
    "href": "posts/lint-rcpp/index.html#turning-off-linting-and-formatting",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Turning off linting and formatting",
    "text": "Turning off linting and formatting\nThere are cases in which we might want to turn linting and formatting off. This might be when the linter does not agree with valid C++ code required in the project, or when the linters and stylers do not agree with each other. These tools are developed separately by large software projects with their own internal requirements, and solutions to issues encountered in their work: clang-format by LLVM (although specifying -style=google), and cpplint from Google’s work.\n\nLinter-enforced paradigms\nSometimes, the linter or styler developer enforces both a style and the use of certain programming paradigms. An example from cpplint is when it warns against passing function arguments by reference, and prefers for these to be passed as pointers, or as constant references (const int &value).\nint some_function(int &value) { \n  /* operations modifying value */\n  return value;\n}\nPassing the argument as a const reference would not serve the needs of this function, and passing by value is a valid strategy when we don’t want to get into the details of using pointers. (Note that this is typically an issue when large objects such as custom classes or structs are passed to a function multiple times.)\nSimilarly, cpplint will throw a warning about accessing variables using std::move, which is something we encounter in the Newton solver in {finalsize}. While not technically wrong for such a simple use case, the linter is correct to cautiously throw a warning nonetheless.\n\n\nLinter-styler disagreement\nOne example of linter-styler disagreement is the use of BOOST_FOREACH from the Boost libraries as an alternative to for loops. clang-format will insist on adding two spaces before the opening bracket: BOOST_FOREACH  (). cpplint will insist on removing one space.\ncpplint and clang-format also disagree on the order of header inclusions, especially when both local and system headers are included.\n\n\nDisabling checks on code chunks\nEither of these cases could require disabling linting or formatting on some part of the code. It is possible to turn off linting using cpplint at particular lines using the comment // NOLINT. Multiple lines can be protected from linting as well.\n// NOLINTBEGIN\n&lt;some C++ code here&gt;\n// NOLINTEND\nAlternatively, clang-format can be instructed to ignore chunks of code using comment messages too.\n// clang-format off\n&lt;some C++ code here&gt;\n// clang-format on"
  },
  {
    "objectID": "posts/lint-rcpp/index.html#linter-options-for-future-packages",
    "href": "posts/lint-rcpp/index.html#linter-options-for-future-packages",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Linter options for future packages",
    "text": "Linter options for future packages\n{finalsize} is a relatively simple {Rcpp} package, with no C/C++ headers, and no C++ tests. However, future Epiverse-TRACE packages could be more similar to {fluEvidenceSynthesis}, and will have header files, and could also have C++ unit tests via the catch framework.\ncpplint will demand that all local headers be prefixed with their directory (src/), but this would cause the code to break as {Rcpp} looks for a subdirectory called src/src/. This can be turned off by passing the filter option --filter=\"-build/include_subdir\" to cpplint. Alternatively, we could place headers in a subdirectory such as inst/include.\nBoth cpplint and cppcheck can be instructed to ignore C++ test files using the catch testing framework provided by {testthat}. This prevents errors due to the specialised syntax provided by {testthat} in testthat.h, such as context.\n# for cpplint, add an extra exclude statement\ncpplint &lt;...&gt; --exclude=\"src/test*.cpp\" src/*.cpp\n\n# for cppcheck, suppress checks on test files\ncppcheck &lt;...&gt; --suppress=*:src/test_*.cpp src"
  },
  {
    "objectID": "posts/lint-rcpp/index.html#conclusion",
    "href": "posts/lint-rcpp/index.html#conclusion",
    "title": "Improving the C++ Code Quality of an Rcpp Package",
    "section": "Conclusion",
    "text": "Conclusion\nIt is actually somewhat surprising that there does not seem to be a canonical linter for C++ code in {Rcpp} packages. The methods laid out here are an initial implementation developed for use with the {finalsize} package, and the considerations here are a starting point. We shall be continuously evaluating how we ensure the quality of our C++ code as we encounter more use cases while developing future Epiverse-TRACE packages."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "From disconnected elements to a harmonious ecosystem: The Epiverse-TRACE project\n\n\n\n\n\n\n\ndata science\n\n\npipelines\n\n\ninteroperability\n\n\ncommunity\n\n\ndeRSE23\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nHugo Gruson\n\n\n\n\n\n\n  \n\n\n\n\nEpiverse-TRACE Winter 2023 showcase\n\n\n\n\n\n\n\nepiparameter\n\n\nfinalsize\n\n\nepisoap\n\n\nshowcase\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2023\n\n\nAnna Carnegie, Pratik Gupte, Joshua Lambert, Hugo Gruson\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Epiverse-TRACE developer space",
    "section": "",
    "text": "This is the developer space of the Epiverse-TRACE project, where we share opinions and investigations in R package development, or scientific software development more generally."
  }
]